{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8446ba7d",
   "metadata": {},
   "source": [
    "Note: all \".to(device)\" are deleted because Apple M1 chip does does support it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83af57b",
   "metadata": {},
   "source": [
    "Would recommend using Google Colab instead because it will be much faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e884123b",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#env\" data-toc-modified-id=\"env-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>env</a></span><ul class=\"toc-item\"><li><span><a href=\"#d22:-no-noise;-fixed-wgain\" data-toc-modified-id=\"d22:-no-noise;-fixed-wgain-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>d22: no noise; fixed wgain</a></span></li></ul></li><li><span><a href=\"#make-LSTM-agent\" data-toc-modified-id=\"make-LSTM-agent-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>make LSTM agent</a></span></li><li><span><a href=\"#TRAIN\" data-toc-modified-id=\"TRAIN-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>TRAIN</a></span></li><li><span><a href=\"#Test\" data-toc-modified-id=\"Test-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Test</a></span><ul class=\"toc-item\"><li><span><a href=\"#test-and-save-variables\" data-toc-modified-id=\"test-and-save-variables-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>test and save variables</a></span></li></ul></li><li><span><a href=\"#Animation\" data-toc-modified-id=\"Animation-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Animation</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06f9c7f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from LSTM_functions import*\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pickle\n",
    "import gym\n",
    "from gym import spaces, Env\n",
    "from gym.spaces import Dict, Box\n",
    "import torch\n",
    "from numpy import pi\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import pandas as pd\n",
    "import math\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.linalg import vector_norm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c042e70",
   "metadata": {},
   "source": [
    "# env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bd3207",
   "metadata": {},
   "source": [
    "## d22: no noise; fixed wgain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79fce348",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"LSTM_data/LSTM_July_26_2\"\n",
    "\n",
    "## Reward: 12480 after training for 3000 + episodes\n",
    "\n",
    "## Reward: 12100 for e3 env. Lol....not much of a difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "140242e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " episode:  0\n",
      "flash_on_interval:  2.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma= 0.995\n",
    "soft_q_lr = 0.0015\n",
    "policy_lr = 0.003  \n",
    "alpha_lr = 0.002 \n",
    "update_itr= 1\n",
    "hidden_dim= 128\n",
    "reward_scale= 10\n",
    "target_entropy= -2\n",
    "soft_tau= 0.015\n",
    "\n",
    "\n",
    "# obs = [angle, distance, memory, angle2, distance2, memory2, ...]\n",
    "# before = [angle, angle2 ..., distance, distance2, ... memory, memory2]\n",
    "\n",
    "\n",
    "reward_per_episode = 0\n",
    "\n",
    "class MultiFF(Env):\n",
    "  def __init__(self):\n",
    "      super(MultiFF, self).__init__()\n",
    "      self.num_ff = 200\n",
    "      self.arena_radius = 1000\n",
    "      self.episode_len = 1024 \n",
    "      self.dt = 0.25 \n",
    "      #self.current_episode = 0\n",
    "      #self.action_space = spaces.Box(low=-1., high=1., shape=(2,),dtype=np.float32)\n",
    "      self.action_space = spaces.Box(low=-1., high=1., shape=(2,),dtype=np.float32)\n",
    "      self.obs_ff = 2\n",
    "      self.observation_space = spaces.Box(low=-1., high=1., shape=(self.obs_ff*3,),dtype=np.float32)\n",
    "      self.terminal_vel = 0.01\n",
    "      #self.pro_noise_std = 0.005\n",
    "      self.vgain = 200\n",
    "      self.wgain = pi/4\n",
    "      self.reward_per_ff = 100\n",
    "      #self.time_cost = 0.005\n",
    "      #self.total_time = 0\n",
    "      #self.zero_action = False\n",
    "      #self.target_update_counter_num = 20\n",
    "      #self.reward_per_episode = []\n",
    "      #self.update_slots = True\n",
    "      #self.closest_ff_distance = 200\n",
    "      self.pro_noise_std = 0.005\n",
    "      self.epi_num = -1\n",
    "      self.has_sped_up_before = False\n",
    "      self.invisible_distance = 400\n",
    "      self.invisible_angle = 2*pi/9\n",
    "      self.reward_boundary = 25\n",
    "      # self.dev_v_cost = 1\n",
    "      self.flash_on_interval = 2.1\n",
    "      # self.distance_noise_factor = 1/6\n",
    "      # self.distance_noise_constant = 5\n",
    "      # self.angle_noise_factor = 1/8\n",
    "      # self.angle_noise_constant = 1/8\n",
    "      self.num_intervals = 1500\n",
    "      self.distance2center_cost = 2\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  def reset(self):\n",
    "    self.epi_num += 1\n",
    "    print(\"\\n episode: \", self.epi_num)\n",
    "    self.num_targets = 0\n",
    "    #self.past_speeds = []\n",
    "    self.time = 0\n",
    "    #self.counter = 0\n",
    "    #self.current_target_index = torch.tensor([999], dtype=torch.int32)\n",
    "    #self.previous_target_index = self.current_target_index\n",
    "    self.ff_flash = []\n",
    "    self.has_sped_up_before = False\n",
    "    \n",
    "\n",
    "    if self.flash_on_interval > 0.31:\n",
    "      global reward_per_episode\n",
    "      if reward_per_episode >= 1000:\n",
    "        self.flash_on_interval-=0.3 \n",
    "        reward_per_episode = 800\n",
    "        global best_cum_rewards\n",
    "        best_cum_rewards = best_cum_rewards/2\n",
    "    else:\n",
    "      self.distance2center_cost = 0\n",
    "    print(\"flash_on_interval: \",   self.flash_on_interval)\n",
    "\n",
    "    for i in range(self.num_ff):\n",
    "      \n",
    "      first_flash = torch.rand(1)\n",
    "      intervals = torch.poisson(torch.ones(self.num_intervals-1)*3)\n",
    "      t0 = torch.cat((first_flash, first_flash+torch.cumsum(intervals, dim=0)+torch.cumsum(torch.ones(self.num_intervals-1)*0.3, dim=0)))\n",
    "      t1 = t0 + torch.ones(self.num_intervals)*self.flash_on_interval\n",
    "      self.ff_flash.append(torch.stack((t0, t1), dim=1))\n",
    "\n",
    "    self.ffr = torch.sqrt(torch.rand(self.num_ff))*self.arena_radius # The radius of the arena changed from 1000 cm/s to 200 cm/s\n",
    "    self.fftheta = torch.rand(self.num_ff)*2*pi\n",
    "    #self.ffrt = torch.stack((self.ffr, self.fftheta), dim=1) \n",
    "    self.ffx = torch.cos(self.fftheta) * self.ffr\n",
    "    self.ffy = torch.sin(self.fftheta) * self.ffr\n",
    "    self.ffxy = torch.stack((self.ffx, self.ffy), dim=1)\n",
    "    self.ffx2 = self.ffx.clone()\n",
    "    self.ffy2 = self.ffy.clone()\n",
    "    self.ffxy2 = torch.stack((self.ffx2, self.ffy2), dim=1)\n",
    "    #self.ff_info={}\n",
    "    #self.update_slots = True\n",
    "    self.agentx = torch.tensor([0])\n",
    "    self.agenty = torch.tensor([0])\n",
    "    self.agentr = torch.zeros(1)\n",
    "    self.agentxy = torch.tensor([0, 0])\n",
    "    self.agentheading = torch.zeros(1).uniform_(0, 2*pi)\n",
    "    self.dv = torch.zeros(1).uniform_(-0.05, 0.05) \n",
    "    self.dw = torch.zeros(1)\n",
    "    self.end_episode = False \n",
    "    self.obs = self.beliefs().numpy()\n",
    "    #self.chunk_50s = 1\n",
    "    self.episode_reward = 0\n",
    "    #self.stop_rewarding_speed = False\n",
    "    #self.current_obs_steps = 0\n",
    "    return self.obs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  def calculate_reward(self):\n",
    "      #action_cost=((self.previous_action[1]-self.action[1])**2+(self.previous_action[0]-self.action[0])**2)*self.mag_cost\n",
    "      # To incorporate action_cost, we need to incorporate previous_action into decision_info\n",
    "      #self.total_time += 1\n",
    "      # In addition to rewarding the monkey for capturing the firefly, we also use different phases of rewards to teach monkey specific behaviours\n",
    "      #reward = -self.time_cost\n",
    "      reward = 0\n",
    "      # Reward shaping\n",
    "      # Phase I: reward the agent for learning to stop \n",
    "      # Always: reward the agent for capturing fireflies\n",
    "      self.num_targets = 0\n",
    "      if abs(self.sys_vel[1]) <= self.terminal_vel:\n",
    "        captured_ff_index0 = (self.ff_distance_all <= self.reward_boundary).nonzero().reshape(-1)\n",
    "        total_deviated_distance = torch.sum(self.ff_distance_all[captured_ff_index0]).item()\n",
    "        captured_ff_index = captured_ff_index0.tolist()\n",
    "        self.captured_ff_index = captured_ff_index\n",
    "        num_targets = len(captured_ff_index)\n",
    "        self.num_targets = num_targets\n",
    "        if num_targets > 0: # If the monkey hs captured at least 1 ff\n",
    "          # Calculate reward\n",
    "          reward = reward + self.reward_per_ff * num_targets - total_deviated_distance*self.distance2center_cost \n",
    "          # Replace the captured ffs with ffs of new locations\n",
    "          self.ffr[captured_ff_index]= torch.sqrt(torch.rand(num_targets))*self.arena_radius\n",
    "          self.fftheta[captured_ff_index]= torch.rand(num_targets)*2*pi\n",
    "          #self.ffrt = torch.stack((self.ffr, self.fftheta), dim=1) \n",
    "          self.ffx[captured_ff_index] = torch.cos(self.fftheta[captured_ff_index]) * self.ffr[captured_ff_index]\n",
    "          self.ffy[captured_ff_index] = torch.sin(self.fftheta[captured_ff_index]) * self.ffr[captured_ff_index]\n",
    "          self.ffxy = torch.stack((self.ffx, self.ffy), dim=1)\n",
    "          # Delete the information from self.ff_info\n",
    "          #[self.ff_info.pop(key) for key in captured_ff_index if (key in self.ff_info)]\n",
    "          #self.current_target_index = torch.tensor([999], dtype=torch.int32)\n",
    "          #self.previous_target_index = self.current_target_index\n",
    "          # Reward the firefly based on the average speed before capturing the firefly\n",
    "          ##reward = reward + sum(self.past_speeds)/len(self.past_speeds)\n",
    "          #print(round(self.time, 2), \"sys_vel: \", [round(i, 4) for i in self.sys_vel.tolist()], \"obs: \", list(np.round(self.obs, decimals = 2)), \"n_targets: \",  num_targets)\n",
    "          print(round(self.time, 2), \"sys_vel: \", [round(i, 4) for i in self.sys_vel.tolist()], \"n_targets: \",  num_targets)\n",
    "          #self.update_slots = True\n",
    "          #self.stop_rewarding_speed = True\n",
    "        #elif self.has_sped_up_before == True:\n",
    "        # based on Ruiyi's formula, using the distance of the closest ff in obs\n",
    "        #reward += math.exp(-((self.ff_current[1, 0]**2)*(25/1.5)**2)/2)\n",
    "        #self.has_sped_up_before = False\n",
    "      self.prev_action = self.action.clone()\n",
    "\n",
    "      return reward\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  def step(self, action):\n",
    "    self.time += self.dt\n",
    "    action=torch.tensor(action)\n",
    "    self.action = action.clone()\n",
    "    action[1] = action[1]/2+0.5\n",
    "    self.sys_vel=action.clone()\n",
    "    self.state_step(action)\n",
    "    self.obs = self.beliefs().numpy()\n",
    "    reward=self.calculate_reward()\n",
    "    self.episode_reward += reward\n",
    "    \n",
    "    if self.time >= self.episode_len*self.dt:\n",
    "      self.end_episode = True\n",
    "      #self.current_episode += 1\n",
    "      print(\"Reward for the episode: \", self.episode_reward)\n",
    "    #print(\"action: \", torch.round(action, decimals = 3), \"obs: \", np.round(self.obs, decimals = 3), \"Reward: \", round(reward, 3))\n",
    "    return self.obs, reward, self.end_episode, {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  def state_step(self,action):\n",
    "    #vnoise=torch.distributions.Normal(0,torch.ones([1,1])).sample()*self.pro_noise_std \n",
    "    #wnoise=torch.distributions.Normal(0,torch.ones([1,1])).sample()*self.pro_noise_std\n",
    "    vnoise = 0\n",
    "    wnoise = 0\n",
    "    self.dw_normed = (action[0]+wnoise)\n",
    "    self.dv_normed = (action[1]+vnoise)\n",
    "    self.dw = (action[0]+wnoise)*self.wgain*self.dt\n",
    "    self.agentheading = self.agentheading + self.dw.item()\n",
    "    self.dv =  (action[1]+vnoise)*self.vgain*self.dt\n",
    "    self.dx= torch.cos(self.agentheading)*self.dv\n",
    "    self.dy= torch.sin(self.agentheading)*self.dv\n",
    "    self.agentx = self.agentx + self.dx.item()\n",
    "    self.agenty = self.agenty + self.dy.item()\n",
    "    self.agentxy = torch.cat((self.agentx, self.agenty))\n",
    "    self.agentr = vector_norm(self.agentxy)\n",
    "    self.agenttheta = torch.atan2(self.agenty, self.agentx)  \n",
    "                               \n",
    "    if self.agentr >= self.arena_radius:\n",
    "      self.agentr = 2*self.arena_radius-self.agentr\n",
    "      self.agenttheta = self.agenttheta + pi\n",
    "      self.agentx = (self.agentr*torch.cos(self.agenttheta)).reshape(1,)\n",
    "      self.agenty = (self.agentr*torch.sin(self.agenttheta)).reshape(1,)\n",
    "      self.agentxy = torch.cat((self.agentx, self.agenty))\n",
    "      self.agentheading = self.agenttheta - pi\n",
    "    while self.agentheading >= 2*pi:\n",
    "      self.agentheading = self.agentheading - 2*pi\n",
    "    while self.agentheading < 0:\n",
    "      self.agentheading = self.agentheading + 2*pi   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  def beliefs(self): \n",
    "\n",
    "\n",
    "    # Make a tensor containing the relative distance of all fireflies to the agent\n",
    "    self.ff_distance_all = vector_norm(self.ffxy - self.agentxy, dim=1)\n",
    "    #distance_noise = torch.distributions.Normal(0,self.ff_distance_all*self.distance_noise_factor+ self.distance_noise_constant).sample() \n",
    "    #self.ff_distance_all = self.ff_distance_all + distance_noise\n",
    "    # Make a tensor containing the relative (real) angle of all fireflies to the agent\n",
    "    ffradians = torch.atan2(self.ffy-self.agenty, self.ffx-self.agentx)\n",
    "    angle0 = ffradians - self.agentheading\n",
    "    angle0[angle0 > pi] = angle0[angle0 > pi] - 2*pi\n",
    "    angle0[angle0 < -pi] = angle0[angle0 < -pi] + 2*pi\n",
    "\n",
    "    #angle_noise = torch.distributions.Normal(0,torch.abs(angle0)*self.angle_noise_factor+ self.angle_noise_constant).sample() \n",
    "    #angle0 = angle0+angle_noise\n",
    "    # Adjust the angle based on reward boundary\n",
    "    angle1 = torch.abs(angle0)-torch.abs(torch.arcsin(torch.div(self.reward_boundary, torch.clip( self.ff_distance_all, self.reward_boundary, 400) ))) # use torch clip to get valid arcsin input\n",
    "    angle2 = torch.clip(angle1,0,pi)\n",
    "    ff_angle_all = torch.sign(angle0)* angle2\n",
    "    # Update the tensor containing the uncertainties of all fireflies to the agent\n",
    "    visible_ff = torch.logical_and( self.ff_distance_all < self.invisible_distance, torch.abs(angle0) < self.invisible_angle)\n",
    "    self.visible_ff_indices0 = visible_ff.nonzero().reshape(-1)\n",
    "    for index in self.visible_ff_indices0:\n",
    "      ff = self.ff_flash[index]\n",
    "      if not torch.any(torch.logical_and(ff[:, 0] <= self.time, ff[:, 1] >= self.time)):\n",
    "        visible_ff[index] = False\n",
    "    self.visible_ff_indices = visible_ff.nonzero().reshape(-1)\n",
    "\n",
    "    # Consider the case where there are fewer than self.obs_ff fireflies that are in memory\n",
    "    if torch.numel(self.visible_ff_indices) >= self.obs_ff:\n",
    "      # Rank the ff whose \"memory\" is creater than 0 based on distance\n",
    "      sorted_indices =  torch.topk(-self.ff_distance_all[self.visible_ff_indices], self.obs_ff).indices\n",
    "      self.topk_indices = self.visible_ff_indices[sorted_indices]\n",
    "      self.ffxy_topk = self.ffxy[self.topk_indices]\n",
    "      #self.ff_distance_topk = vector_norm(self.ffxy_topk - self.agentxy, dim=1)\n",
    "      self.ff_distance_topk = self.ff_distance_all[self.topk_indices]\n",
    "      self.ff_angle_topk_2 = angle0[self.topk_indices]\n",
    "      ff_angle_topk_3 = ff_angle_all[self.topk_indices]\n",
    "\n",
    "      # # Calculate relative angles \n",
    "      # ffradians = torch.atan2(self.ffxy_topk[:,1]-self.agenty, self.ffxy_topk[:,0]-self.agentx)\n",
    "      # angle0 = ffradians - self.agentheading\n",
    "      # angle0[angle0 > pi] = angle0[angle0 > pi] - 2*pi\n",
    "      # angle0[angle0 < -pi] = angle0[angle0 < -pi] + 2*pi\n",
    "      # self.ff_angle_topk_2 = angle0.clone()\n",
    "     \n",
    "      # # Calculate relative angles of all ffs based on reward boundaries\n",
    "      # # Adjust the angle based on reward boundary\n",
    "      # angle1 = torch.abs(angle0)-torch.abs(torch.arcsin(torch.div(self.reward_boundary, torch.clip(self.ff_distance_topk, self.reward_boundary, 400) ))) # use torch clip to get valid arcsin input\n",
    "      # angle2 = torch.clip(angle1,0,pi)\n",
    "      # ff_angle_topk_3 = torch.sign(angle0)* angle2\n",
    "      \n",
    "      \n",
    "      # Concatenate distance, angle, and memory\n",
    "      ff_array = torch.stack((self.ff_angle_topk_2, ff_angle_topk_3, self.ff_distance_topk), dim=0)\n",
    "    \n",
    "\n",
    "    elif torch.numel(self.visible_ff_indices) == 0:\n",
    "      ff_array = torch.tensor([[0], [0], [self.invisible_distance]]).repeat([1, self.obs_ff])\n",
    "      self.topk_indices = torch.tensor([])\n",
    "\n",
    "    else:\n",
    "      sorted_distance, sorted_indices = torch.sort(-self.ff_distance_all[self.visible_ff_indices])\n",
    "      self.topk_indices = self.visible_ff_indices[sorted_indices]\n",
    "      self.ffxy_topk = self.ffxy[self.topk_indices ]\n",
    "      \n",
    "      self.ff_distance_topk = self.ff_distance_all[self.topk_indices]\n",
    "      self.ff_angle_topk_2 = angle0[self.topk_indices]\n",
    "      ff_angle_topk_3 = ff_angle_all[self.topk_indices]\n",
    "      # self.ff_distance_topk = vector_norm(self.ffxy_topk - self.agentxy, dim=1)\n",
    "      # # Calculate relative angles \n",
    "      # ffradians = torch.atan2(self.ffxy_topk[:,1]-self.agenty, self.ffxy_topk[:,0]-self.agentx)\n",
    "      # angle0 = ffradians - self.agentheading\n",
    "      # angle0[angle0 > pi] = angle0[angle0 > pi] - 2*pi\n",
    "      # angle0[angle0 < -pi] = angle0[angle0 < -pi] + 2*pi\n",
    "      # self.ff_angle_topk_2 = angle0.clone()\n",
    "      # # Calculate relative angles of all ffs based on reward boundaries\n",
    "      # # Adjust the angle based on reward boundary\n",
    "      # angle1 = torch.abs(angle0)-torch.abs(torch.arcsin(torch.div(self.reward_boundary, torch.clip(self.ff_distance_topk, self.reward_boundary, 400) ))) # use torch clip to get valid arcsin input\n",
    "      # angle2 = torch.clip(angle1,0,pi)\n",
    "      # ff_angle_topk_3 = torch.sign(angle0)* angle2\n",
    "      \n",
    "      # Concatenate distance, angle, and memory\n",
    "      ff_array0 = torch.stack((self.ff_angle_topk_2, ff_angle_topk_3, self.ff_distance_topk), dim=0)\n",
    "      needed_ff = self.obs_ff - torch.numel(self.visible_ff_indices)\n",
    "      ff_array = torch.stack([ff_array0.reshape([3,-1]), torch.tensor([[0], [0], [self.invisible_distance]]).repeat([1, needed_ff])], dim=1)\n",
    "      \n",
    "\n",
    "    # ff_array[0:2,:] = ff_array[0:2,:]/pi\n",
    "    # ff_array[2,:] = (ff_array[2,:]/self.invisible_distance-0.5)*2\n",
    "    ff_array[2,:] = ff_array[2,:]/self.invisible_distance\n",
    "    self.ff_array = ff_array.clone()\n",
    "    return torch.flatten(ff_array.transpose(0, 1))\n",
    "\n",
    "\n",
    "env = MultiFF()  \n",
    "env.reset()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe9a1be",
   "metadata": {},
   "source": [
    "# make LSTM agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "300fa425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dusiyi/.conda/envs/Multifirefly-Project/lib/python3.9/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft Q Network (1,2):  QNetworkLSTM2(\n",
      "  (linear1): Linear(in_features=10, out_features=128, bias=True)\n",
      "  (lstm1): LSTM(128, 128, dropout=0.2)\n",
      "  (linear2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Policy Network:  SAC_PolicyNetworkLSTM(\n",
      "  (linear1): Linear(in_features=6, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=8, out_features=128, bias=True)\n",
      "  (lstm1): LSTM(128, 128, dropout=0.2)\n",
      "  (linear3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (mean_linear): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (log_std_linear): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "replay_buffer_size = 100\n",
    "batch_size = 10\n",
    "replay_buffer = ReplayBufferLSTM2(replay_buffer_size)\n",
    "\n",
    "# choose env\n",
    "env = MultiFF()\n",
    "action_space = env.action_space\n",
    "state_space  = env.observation_space\n",
    "action_range=1.\n",
    "\n",
    "action_dim = action_space.shape[0]\n",
    "\n",
    "# hyper-parameters for RL training\n",
    "max_episodes  = 1000\n",
    "max_steps   = 1024\n",
    "frame_idx   = 0\n",
    "AUTO_ENTROPY=True\n",
    "DETERMINISTIC=False\n",
    "rewards     = []\n",
    "train_freq = 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sac_trainer=SAC_Trainer(replay_buffer, state_space, action_space, hidden_dim=hidden_dim, action_range=action_range,\\\n",
    "                          gamma = gamma, soft_q_lr = soft_q_lr, policy_lr = policy_lr, alpha_lr = alpha_lr, \\\n",
    "                          batch_size = batch_size, update_itr = update_itr, reward_scale = reward_scale, \\\n",
    "                        target_entropy = target_entropy, soft_tau = soft_tau, train_freq = train_freq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12130c9",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fc0feb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " episode:  18\n",
      "flash_on_interval:  1.8\n",
      "0.25 sys_vel:  [-0.9999, 0.0] n_targets:  1\n",
      "6.75 sys_vel:  [1.0, 0.0001] n_targets:  1\n",
      "14.25 sys_vel:  [-1.0, 0.0] n_targets:  1\n",
      "17.25 sys_vel:  [-0.9461, 0.0] n_targets:  1\n",
      "19.25 sys_vel:  [-0.9956, 0.0] n_targets:  1\n",
      "19.75 sys_vel:  [-1.0, 0.0] n_targets:  1\n",
      "33.0 sys_vel:  [-0.9758, 0.0] n_targets:  1\n",
      "36.25 sys_vel:  [-0.6922, 0.0] n_targets:  1\n",
      "42.25 sys_vel:  [-1.0, 0.0] n_targets:  1\n",
      "59.5 sys_vel:  [0.6498, 0.0] n_targets:  1\n",
      "68.0 sys_vel:  [0.6867, 0.0] n_targets:  1\n",
      "90.0 sys_vel:  [-1.0, 0.0] n_targets:  1\n",
      "96.75 sys_vel:  [-1.0, 0.0] n_targets:  1\n",
      "130.75 sys_vel:  [0.9883, 0.0002] n_targets:  1\n",
      "189.75 sys_vel:  [-1.0, 0.0] n_targets:  1\n",
      "194.75 sys_vel:  [-1.0, 0.0] n_targets:  1\n",
      "231.75 sys_vel:  [-1.0, 0.0009] n_targets:  1\n",
      "235.75 sys_vel:  [-0.9966, 0.0008] n_targets:  1\n",
      "238.25 sys_vel:  [-1.0, 0.0] n_targets:  1\n",
      "254.25 sys_vel:  [0.9975, 0.0] n_targets:  1\n",
      "Reward for the episode:  1306.4972229003906\n",
      "Episode:  0 | Episode Reward:  1306.4972229003906\n",
      "\n",
      " episode:  19\n",
      "flash_on_interval:  1.8\n",
      "16.5 sys_vel:  [-0.9994, 0.0014] n_targets:  1\n",
      "21.0 sys_vel:  [-1.0, 0.0006] n_targets:  1\n",
      "24.5 sys_vel:  [-0.4353, 0.0] n_targets:  1\n",
      "37.5 sys_vel:  [-1.0, 0.0] n_targets:  1\n",
      "83.75 sys_vel:  [-1.0, 0.0] n_targets:  1\n",
      "90.25 sys_vel:  [-1.0, 0.0] n_targets:  1\n",
      "106.0 sys_vel:  [-0.9481, 0.0] n_targets:  1\n",
      "170.0 sys_vel:  [-0.2837, 0.0006] n_targets:  1\n",
      "197.0 sys_vel:  [-0.9999, 0.0] n_targets:  1\n",
      "214.75 sys_vel:  [-1.0, 0.0] n_targets:  1\n",
      "233.75 sys_vel:  [-1.0, 0.0] n_targets:  1\n",
      "246.5 sys_vel:  [-1.0, 0.008] n_targets:  1\n",
      "250.75 sys_vel:  [-1.0, 0.0] n_targets:  1\n",
      "Reward for the episode:  903.2201800346375\n",
      "Episode:  1 | Episode Reward:  903.2201800346375\n",
      "\n",
      " episode:  20\n",
      "flash_on_interval:  1.8\n",
      "0.25 sys_vel:  [-1.0, 0.0] n_targets:  2\n",
      "72.25 sys_vel:  [-1.0, 0.0001] n_targets:  1\n",
      "74.75 sys_vel:  [-1.0, 0.0005] n_targets:  1\n",
      "109.25 sys_vel:  [-1.0, 0.005] n_targets:  1\n",
      "110.75 sys_vel:  [0.7642, 0.0079] n_targets:  1\n",
      "119.25 sys_vel:  [-0.9978, 0.0] n_targets:  1\n",
      "126.0 sys_vel:  [-1.0, 0.0002] n_targets:  1\n",
      "132.0 sys_vel:  [-1.0, 0.0] n_targets:  1\n",
      "238.5 sys_vel:  [-1.0, 0.0028] n_targets:  1\n",
      "251.75 sys_vel:  [-1.0, 0.0] n_targets:  1\n",
      "Reward for the episode:  708.6585655212402\n",
      "Episode:  2 | Episode Reward:  708.6585655212402\n",
      "\n",
      " episode:  21\n",
      "flash_on_interval:  1.8\n",
      "11.0 sys_vel:  [-1.0, 0.0] n_targets:  1\n",
      "22.75 sys_vel:  [-1.0, 0.0] n_targets:  1\n",
      "31.0 sys_vel:  [-1.0, 0.0] n_targets:  1\n",
      "46.25 sys_vel:  [-1.0, 0.0001] n_targets:  1\n",
      "51.0 sys_vel:  [-1.0, 0.0] n_targets:  1\n",
      "65.5 sys_vel:  [-1.0, 0.0] n_targets:  1\n",
      "69.0 sys_vel:  [-1.0, 0.0] n_targets:  1\n",
      "81.75 sys_vel:  [-1.0, 0.0] n_targets:  1\n",
      "128.0 sys_vel:  [-1.0, 0.0] n_targets:  1\n",
      "134.0 sys_vel:  [-1.0, 0.0] n_targets:  1\n",
      "186.5 sys_vel:  [-1.0, 0.0] n_targets:  1\n",
      "196.25 sys_vel:  [-1.0, 0.0] n_targets:  1\n",
      "210.25 sys_vel:  [-1.0, 0.0058] n_targets:  1\n",
      "212.5 sys_vel:  [-1.0, 0.0052] n_targets:  1\n"
     ]
    }
   ],
   "source": [
    "best_avg_rewards = -100\n",
    "\n",
    "# training loop\n",
    "#sac_trainer.load_model(model_path)\n",
    "max_episodes  = 10000\n",
    "eval_episodes = 2\n",
    "best_cum_rewards = -1000000\n",
    "\n",
    "\n",
    "sac_trainer.soft_q_net1.train()\n",
    "sac_trainer.soft_q_net2.train()\n",
    "sac_trainer.policy_net.train()\n",
    "\n",
    "eval_counter = 0\n",
    "eval_rewards = []\n",
    "\n",
    "for eps in range(max_episodes):\n",
    "    state =  env.reset()\n",
    "    last_action = env.action_space.sample()\n",
    "    episode_state = []\n",
    "    episode_action = []\n",
    "    episode_last_action = []\n",
    "    episode_reward = []\n",
    "    episode_next_state = []\n",
    "    episode_done = []\n",
    "    hidden_out = (torch.zeros([1, 1, hidden_dim], dtype=torch.float).to(device), \\\n",
    "        torch.zeros([1, 1, hidden_dim], dtype=torch.float).to(device))  # initialize hidden state for LSTM, (hidden, cell), each is (layer, batch, dim)             \n",
    "    for step in range(max_steps):\n",
    "        hidden_in = hidden_out\n",
    "\n",
    "        action, hidden_out = sac_trainer.policy_net.get_action(state, last_action, hidden_in, deterministic = False)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # env.render()       \n",
    "            \n",
    "        if step == 0:\n",
    "            ini_hidden_in = hidden_in\n",
    "            ini_hidden_out = hidden_out\n",
    "        episode_state.append(state)\n",
    "        episode_action.append(action)\n",
    "        episode_last_action.append(last_action)\n",
    "        episode_reward.append(reward)\n",
    "        episode_next_state.append(next_state)\n",
    "        episode_done.append(done) \n",
    "\n",
    "        state = next_state\n",
    "        last_action = action\n",
    "        \n",
    "        if step % train_freq == 0:\n",
    "          if len(replay_buffer) > batch_size:\n",
    "              for i in range(update_itr):\n",
    "                  _=sac_trainer.update(batch_size, reward_scale=10., auto_entropy=AUTO_ENTROPY, target_entropy=-1.*action_dim)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    replay_buffer.push(ini_hidden_in, ini_hidden_out, episode_state, episode_action, episode_last_action, \\\n",
    "        episode_reward, episode_next_state, episode_done)\n",
    "\n",
    "    if eps % 15 == 0 and eps>0: # plot and model saving interval\n",
    "        #np.save('rewards_LSTM', rewards)\n",
    "\n",
    "        #evaluate\n",
    "        eval_env = MultiFF()\n",
    "        cum_rewards = 0        \n",
    "        for eval_eps in range(eval_episodes):\n",
    "            state =  env.reset()\n",
    "            last_action = env.action_space.sample()\n",
    "            hidden_out = (torch.zeros([1, 1, hidden_dim], dtype=torch.float).to(device), \\\n",
    "                torch.zeros([1, 1, hidden_dim], dtype=torch.float).to(device))  # initialize hidden state for LSTM, (hidden, cell), each is (layer, batch, dim)              \n",
    "            for step in range(max_steps):\n",
    "                hidden_in = hidden_out\n",
    "                action, hidden_out = sac_trainer.policy_net.get_action(state, last_action, hidden_in, deterministic = DETERMINISTIC)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                cum_rewards = cum_rewards+reward\n",
    "                state = next_state\n",
    "                last_action = action\n",
    "        reward_per_episode = cum_rewards/eval_episodes\n",
    "        print(\"                      Best avg rewards:\", best_avg_rewards, \"Evaluation: rewards per episode\", reward_per_episode)\n",
    "        if cum_rewards > best_cum_rewards:\n",
    "            best_cum_rewards = cum_rewards\n",
    "            best_avg_rewards = cum_rewards/eval_episodes\n",
    "            sac_trainer.save_model(model_path)\n",
    "            path2 = os.path.join(model_path, 'buffer.pkl')\n",
    "            with open(path2, 'wb') as f:\n",
    "              pickle.dump(replay_buffer.buffer, f)\n",
    "            print(\"                                            Best average rewards = \", best_avg_rewards)\n",
    "            print(\"                                            Best model saved at episode \"+ str(eps))\n",
    "        eval_counter += 1\n",
    "        eval_rewards.append(reward_per_episode)\n",
    "        if (eval_counter > 1):\n",
    "          plt.figure(figsize=(20,5))\n",
    "          plt.plot(eval_rewards)\n",
    "          plt.show()\n",
    "          plt.close()\n",
    "          if len(eval_rewards) > 50:\n",
    "            eval_rewards = eval_rewards[-50:]\n",
    "    print('Episode: ', eps, '| Episode Reward: ', np.sum(episode_reward))\n",
    "    rewards.append(np.sum(episode_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20598964",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bf91d93",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/LSTM_data/July_26_2/buffer.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load buffer\u001b[39;00m\n\u001b[1;32m      2\u001b[0m path2 \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuffer.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m   buffer_buffer \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/LSTM_data/July_26_2/buffer.pkl'"
     ]
    }
   ],
   "source": [
    "# Load buffer\n",
    "path2 = os.path.join(model_path, 'buffer.pkl')\n",
    "with open(path2, 'rb') as f:\n",
    "  buffer_buffer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b89f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.flash_on_interval = 0.3\n",
    "env.distance2center_cost = 0\n",
    "#model_path = \"/content/gdrive/MyDrive/fireflies_agent/lstm/July_18\"\n",
    "sac_trainer.load_model(model_path)\n",
    "#env = MultiFF()\n",
    "last_action = env.action_space.sample()\n",
    "hidden_out = (torch.zeros([1, 1, hidden_dim], dtype=torch.float).to(device), \\\n",
    "        torch.zeros([1, 1, hidden_dim], dtype=torch.float).to(device))  # initialize hidden state for lstm, (hidden, cell), each is (layer, batch, dim)             \n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "eps_num = 5\n",
    "for eps in range(eps_num):\n",
    "\n",
    "    state =  env.reset()\n",
    "    episode_reward = 0\n",
    "    hidden_out = (torch.zeros([1, 1, hidden_dim], dtype=torch.float).to(device), \\\n",
    "        torch.zeros([1, 1, hidden_dim], dtype=torch.float).to(device))  # initialize hidden state for lstm, (hidden, cell), each is (layer, batch, dim)\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        hidden_in = hidden_out\n",
    "        action, hidden_out = sac_trainer.policy_net.get_action(state, last_action, hidden_in, deterministic = DETERMINISTIC)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)   \n",
    "\n",
    "        last_action = action\n",
    "        episode_reward += reward\n",
    "        state=next_state\n",
    "    total_reward += episode_reward\n",
    "    print('Episode: ', eps, '| Episode Reward: ', episode_reward)\n",
    "print(\"Average reward per episode: \", total_reward/eps_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c12f6fd",
   "metadata": {},
   "source": [
    "## test and save variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f54a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path = \"/LSTM_data/July_26_2\" \n",
    "#sac_trainer.load_model(model_path)\n",
    "\n",
    "# Test the trained agent\n",
    "state =  env.reset()\n",
    "last_action = env.action_space.sample()\n",
    "hidden_out = (torch.zeros([1, 1, hidden_dim], dtype=torch.float)#  , \\\n",
    "        torch.zeros([1, 1, hidden_dim], dtype=torch.float)#  )  # initialize hidden state for lstm, (hidden, cell), each is (layer, batch, dim)             \n",
    "episode_reward = 0\n",
    "hidden_out = (torch.zeros([1, 1, hidden_dim], dtype=torch.float)#  , \\\n",
    "    torch.zeros([1, 1, hidden_dim], dtype=torch.float)#  )  # initialize hidden state for lstm, (hidden, cell), each is (layer, batch, dim)\n",
    "n_steps = 700\n",
    "mx = []\n",
    "my = []\n",
    "ffxy_all = []\n",
    "ffxy_visible = []\n",
    "ffxy2_all = []\n",
    "time_rl = []\n",
    "reward_log = []\n",
    "mx_rewarded = []\n",
    "my_rewarded = []\n",
    "mheading = []\n",
    "captured_ff = []\n",
    "current_target = []\n",
    "action_reward = []\n",
    "num_targets = []\n",
    "env_obs = []\n",
    "#captured_ff_cum_x = []\n",
    "#captured_ff_cum_y = []\n",
    "all_captured_ff_x = []\n",
    "all_captured_ff_y = []\n",
    "visible_ff_indices_all = []\n",
    "memory_ff_indices_all = []\n",
    "obs_ff_indices_all = []\n",
    "\n",
    "\n",
    "\n",
    "ff_angles2 = []\n",
    "ff_distances2 = []\n",
    "\n",
    "for step in range(n_steps):\n",
    "  hidden_in = hidden_out\n",
    "  action, hidden_out = sac_trainer.policy_net.get_action(state, last_action, hidden_in, deterministic = DETERMINISTIC)\n",
    "  #print(step, \"Action: \", action)\n",
    "  previous_ffxy = env.ffxy\n",
    "  next_state, reward, done, _ = env.step(action)  \n",
    "  last_action = action\n",
    "  episode_reward += reward\n",
    "  state=next_state\n",
    "  reward_log.append(reward)\n",
    "  num_targets.append(env.num_targets)\n",
    "  #print(\"reward: \", reward)\n",
    "  #print(next_state)\n",
    "  if env.num_targets > 0:\n",
    "    print(step, \"num_targets:\",  env.num_targets)\n",
    "    captured_ff.append(env.captured_ff_index)\n",
    "    all_captured_ff_x = all_captured_ff_x + previous_ffxy[env.captured_ff_index][:,0].tolist()\n",
    "    all_captured_ff_y = all_captured_ff_y + previous_ffxy[env.captured_ff_index][:,1].tolist()\n",
    "  else:\n",
    "    captured_ff.append(0)\n",
    "  #captured_ff_cum_x.append(all_captured_ff_x)\n",
    "  #captured_ff_cum_y.append(all_captured_ff_y)\n",
    "  mx.append(env.agentx.item())\n",
    "  my.append(env.agenty.item())\n",
    "  mheading.append(env.agentheading.item())\n",
    "  time_rl.append(env.time)\n",
    "  ffxy_all.append(env.ffxy.clone())\n",
    "  ffxy2_all.append(env.ffxy2.clone())\n",
    "  ffxy_visible.append(env.ffxy[env.visible_ff_indices].clone())\n",
    "  env_obs.append(obs)\n",
    "  visible_ff_indices_all.append(env.visible_ff_indices)\n",
    "  obs_ff_indices_all.append(env.topk_indices)\n",
    "  if len(env.topk_indices) > 0:\n",
    "    ff_angles2.append(env.ff_angle_topk_2)\n",
    "    ff_distances2.append(env.ff_distance_topk)\n",
    "  else:\n",
    "    ff_angles2.append(torch.tensor([]))\n",
    "    ff_distances2.append(torch.tensor([]))\n",
    "  if done:\n",
    "    obs = env.reset()\n",
    "  #print(step, ffxy_visible[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b7e1c2",
   "metadata": {},
   "source": [
    "# Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3dd60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_dir = \"\" \n",
    "series = 10\n",
    "\n",
    "\n",
    "# Animation 1\n",
    "\n",
    "filename =  f\"Demo{series + 1}\"\n",
    "\n",
    "start = 10\n",
    "num_frame = 200\n",
    "arena_radius = 1000\n",
    "invisible_distance = 250\n",
    "global fig; fig = plt.figure(dpi=100)\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "plt.rcParams['font.size'] = 15\n",
    "colors_YlGn = plt.get_cmap(\"YlGn\")(np.linspace(0,1,101))\n",
    "#colors_RdPu = plt.get_cmap(\"RdPu\")(np.linspace(0,1,101))\n",
    "circle_theta = np.arange(0, 2*pi, 0.01)\n",
    "circle_x = np.cos(circle_theta)*arena_radius\n",
    "circle_y = np.sin(circle_theta)*arena_radius\n",
    "fig, ax = plt.subplots()\n",
    "cum_mx, cum_my = mx[start:start+num_frame], my[start:start+num_frame]\n",
    "xmin, xmax = np.min(cum_mx), np.max(cum_mx)\n",
    "ymin, ymax = np.min(cum_my), np.max(cum_my)\n",
    "ax.set_xlim((xmin-invisible_distance, xmax+invisible_distance))\n",
    "ax.set_ylim((ymin-invisible_distance, ymax+invisible_distance))\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def animate(j):\n",
    "  ax.cla()\n",
    "  ax.axis('off')\n",
    "  ax.plot(circle_x, circle_y)\n",
    "  i = j+start\n",
    "  ax.scatter(mx[start:i+1],my[start:i+1], s=20)\n",
    "  ax.scatter(ffxy_all[i].T[0], ffxy_all[i].T[1], alpha=0.9, c=\"gray\", s=30)\n",
    "\n",
    "  # if torch.numel(ffxy_visible[i]) > 0:\n",
    "  #   ax.scatter(ffxy_visible[i].T[0], ffxy_visible[i].T[1], alpha=0.9, c=\"red\", s=30)\n",
    " \n",
    "  current_obs_ff = obs_ff_indices_all[i]\n",
    "  if torch.numel(current_obs_ff) > 0:\n",
    "    # Plot original positions of ff\n",
    "    ax.scatter(ffxy2_all[i][current_obs_ff].T[0], ffxy2_all[i][current_obs_ff].T[1], s=60, alpha=0.5, color=\"green\")\n",
    "    # Plot position of ffs with noise\n",
    "    ffx_recovered = torch.cos(ff_angles2[i]+mheading[i]) * ff_distances2[i] + mx[i]\n",
    "    ffy_recovered = torch.sin(ff_angles2[i]+mheading[i]) * ff_distances2[i] + my[i]\n",
    "    ax.scatter(ffx_recovered.clone(), ffy_recovered.clone(), alpha=0.9, c=\"black\", s=30)\n",
    "    # Plot the reward boundaries of the circles\n",
    "    for z in range(len(current_obs_ff)):\n",
    "      k = current_obs_ff[z]\n",
    "      # real positions\n",
    "      circle = plt.Circle((ffxy_all[i][k][0], ffxy_all[i][k][1]), 25, facecolor='yellow', edgecolor='orange', alpha=0.3, zorder=1)\n",
    "      ax.add_patch(circle)\n",
    "      # positions with noise\n",
    "      circle = plt.Circle((ffx_recovered[z], ffy_recovered[z]), 25, facecolor='grey', edgecolor='orange', alpha=0.3, zorder=1)\n",
    "      ax.add_patch(circle)\n",
    "  \n",
    "  # Add triangles\n",
    "  ax.plot(np.array([mx[i], mx[i] + 30*np.cos(mheading[i]+2*pi/9)]), np.array([my[i] , my[i] + 30*np.sin(mheading[i]+2*pi/9)]), linewidth = 2)\n",
    "  ax.plot(np.array([mx[i], mx[i] + 30*np.cos(mheading[i]-2*pi/9)]), np.array([my[i] , my[i] + 30*np.sin(mheading[i]-2*pi/9)]), linewidth = 2)\n",
    "  \n",
    "\n",
    "\n",
    "  # Plot ff positions recovered from relative angle and distance \n",
    "  ##if torch.numel(ffx_recovered) > 0: \n",
    "  ##  ffy_recovered = torch.sin(ff_angles2[i]+mheading[i]) * ff_distances2[i] + my[i]\n",
    "  ##  ax.scatter(ffx_recovered.clone(), ffy_recovered.clone(), alpha=0.9, c=\"black\", s=30)\n",
    "\n",
    "  # Plot captured fireflies\n",
    "  if num_targets[i] > 0:\n",
    "    #ax.scatter(ffxy_all[i-1][captured_ff[i]].T[0], ffxy_all[i-1][captured_ff[i]].T[1], s=70, alpha=0.7, color=\"purple\")\n",
    "    #captured_ff_cum_x = captured_ff_cum_x+ffxy_all[i-1][captured_ff[i]].T[0].tolist()\n",
    "    #captured_ff_cum_y = captured_ff_cum_y+ffxy_all[i-1][captured_ff[i]].T[1].tolist()\n",
    "  #if len(captured_ff_cum_x) >0:\n",
    "    ax.scatter(ffxy_all[i-1][captured_ff[i]].T[0], ffxy_all[i-1][captured_ff[i]].T[1], s=100, alpha=0.7, color=\"purple\")\n",
    "  ax.set_xlim((xmin-invisible_distance, xmax+invisible_distance))\n",
    "  ax.set_ylim((ymin-invisible_distance, ymax+invisible_distance))\n",
    "  ax.set_aspect('equal')\n",
    "\n",
    "  #annotation = f\"angle: {round(env_obs[i][0].item(), 2)}\"\n",
    "  #annotation = f\"angle: {round(action_reward[i][0].item(), 2)}\"\n",
    "  #ax.text(0.78, 0.95, annotation, horizontalalignment='left', verticalalignment='top', transform=ax.transAxes, fontsize=12, color=\"black\", bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "anim = animation.FuncAnimation(fig, animate, \n",
    "                frames=num_frame, interval=500, repeat=True) \n",
    "\n",
    "\n",
    "os.makedirs(gif_dir, exist_ok=True)\n",
    "#anim.save(f\"{gif_dir}/{filename}.gif\", writer='pillow', fps=60)\n",
    "\n",
    "#anim.save('1-trial.gif', writer='pillow', fps=60)\n",
    "\n",
    "writervideo = animation.FFMpegWriter(fps=4) \n",
    "anim.save(f\"{gif_dir}/{filename}.mp4\", writer=writervideo)\n",
    "anim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ff_venv",
   "language": "python",
   "name": "ff_venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
