{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "899a1d5c",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#import-packages\" data-toc-modified-id=\"import-packages-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>import packages</a></span></li><li><span><a href=\"#env\" data-toc-modified-id=\"env-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>env</a></span><ul class=\"toc-item\"><li><span><a href=\"#c16.2\" data-toc-modified-id=\"c16.2-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>c16.2</a></span></li></ul></li><li><span><a href=\"#agent\" data-toc-modified-id=\"agent-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>agent</a></span></li><li><span><a href=\"#train\" data-toc-modified-id=\"train-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>train</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9904b001",
   "metadata": {},
   "source": [
    "# import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b690492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/dusiyi/Multifirefly-Project/RL/SB3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fdd43fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from SB3_functions import SaveOnBestTrainingRewardCallback\n",
    "import random\n",
    "import gym\n",
    "from gym import spaces, Env\n",
    "from gym.spaces import Dict, Box\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy import pi\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import pandas as pd\n",
    "import math\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.linalg import vector_norm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "# For animation\n",
    "from matplotlib import rc\n",
    "rc('animation', html='jshtml')\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "np.set_printoptions(suppress=True)\n",
    "from PIL import Image, ImageDraw, ImageOps\n",
    "from IPython.display import Image as Image2\n",
    "import os\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
    "from typing import Any\n",
    "from typing import Dict\n",
    "from matplotlib import rc, cm\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5214f7c7",
   "metadata": {},
   "source": [
    "# env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3dd03f",
   "metadata": {},
   "source": [
    "## c16.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1031c24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"SB3_data/SB3_Aug_11\"\n",
    "#os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# obs = [angle, distance, memory, angle2, distance2, memory2, ...]\n",
    "# before = [angle, angle2 ..., distance, distance2, ... memory, memory2]\n",
    "\n",
    "class MultiFF(Env):\n",
    "    def __init__(self):\n",
    "        super(MultiFF, self).__init__()\n",
    "        self.num_ff = 200\n",
    "        self.arena_radius = 1000\n",
    "        self.episode_len = 16000\n",
    "        self.dt = 0.25\n",
    "        # self.current_episode = 0\n",
    "        # self.action_space = spaces.Box(low=-1., high=1., shape=(2,),dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=-1., high=1., shape=(2,), dtype=np.float32)\n",
    "        self.obs_ff = 2\n",
    "        self.observation_space = spaces.Box(low=-1., high=1., shape=(self.obs_ff * 4,), dtype=np.float32)\n",
    "        self.terminal_vel = 0.01\n",
    "        # self.pro_noise_std = 0.005\n",
    "        self.vgain = 200\n",
    "        self.wgain = pi / 2\n",
    "        self.reward_per_ff = 100\n",
    "        # self.time_cost = 0.005\n",
    "        # self.total_time = 0\n",
    "        # self.zero_action = False\n",
    "        # self.target_update_counter_num = 20\n",
    "        # self.reward_per_episode = []\n",
    "        # self.update_slots = True\n",
    "        # self.closest_ff_distance = 200\n",
    "        self.pro_noise_std = 0.005\n",
    "        self.epi_num = -1\n",
    "        self.has_sped_up_before = False\n",
    "        self.full_memory = 3\n",
    "        self.internal_noise_factor = 3\n",
    "        self.ff_memory_all = torch.ones([self.num_ff, ]) * self.full_memory\n",
    "        self.invisible_distance = 400\n",
    "        self.invisible_angle = 2 * pi / 9\n",
    "        self.reward_boundary = 25\n",
    "\n",
    "    def reset(self):\n",
    "        self.epi_num += 1\n",
    "        print(\"\\n episode: \", self.epi_num)\n",
    "        self.num_targets = 0\n",
    "        # self.past_speeds = []\n",
    "        self.time = 0\n",
    "        # self.counter = 0\n",
    "        # self.current_target_index = torch.tensor([999], dtype=torch.int32)\n",
    "        # self.previous_target_index = self.current_target_index\n",
    "        self.ff_flash = []\n",
    "        self.has_sped_up_before = False\n",
    "\n",
    "        for i in range(self.num_ff):\n",
    "            num_intervals = 1500\n",
    "            first_flash = torch.rand(1)\n",
    "            intervals = torch.poisson(torch.ones(num_intervals - 1) * 3)\n",
    "            t0 = torch.cat((first_flash, first_flash + torch.cumsum(intervals, dim=0) + torch.cumsum(\n",
    "                torch.ones(num_intervals - 1) * 0.3, dim=0)))\n",
    "            t1 = t0 + torch.ones(num_intervals) * 0.3\n",
    "            self.ff_flash.append(torch.stack((t0, t1), dim=1))\n",
    "\n",
    "        self.ffr = torch.sqrt(\n",
    "            torch.rand(self.num_ff)) * self.arena_radius  # The radius of the arena changed from 1000 cm/s to 200 cm/s\n",
    "        self.fftheta = torch.rand(self.num_ff) * 2 * pi\n",
    "        # self.ffrt = torch.stack((self.ffr, self.fftheta), dim=1)\n",
    "        self.ffx = torch.cos(self.fftheta) * self.ffr\n",
    "        self.ffy = torch.sin(self.fftheta) * self.ffr\n",
    "        self.ffxy = torch.stack((self.ffx, self.ffy), dim=1)\n",
    "        self.ffx2 = self.ffx.clone()\n",
    "        self.ffy2 = self.ffy.clone()\n",
    "        self.ffxy2 = torch.stack((self.ffx2, self.ffy2), dim=1)\n",
    "        # self.ff_info={}\n",
    "        # self.update_slots = True\n",
    "        self.agentx = torch.tensor([0])\n",
    "        self.agenty = torch.tensor([0])\n",
    "        self.agentr = torch.zeros(1)\n",
    "        self.agentxy = torch.tensor([0, 0])\n",
    "        self.agentheading = torch.zeros(1).uniform_(0, 2 * pi)\n",
    "        self.dv = torch.zeros(1).uniform_(-0.05, 0.05)\n",
    "        self.dw = torch.zeros(1)\n",
    "        self.end_episode = False\n",
    "        self.obs = self.beliefs().numpy()\n",
    "        # self.chunk_50s = 1\n",
    "        self.episode_reward = 0\n",
    "        # self.stop_rewarding_speed = False\n",
    "        # self.current_obs_steps = 0\n",
    "        self.ff_memory_all = torch.ones([self.num_ff, ])\n",
    "        return self.obs\n",
    "\n",
    "    def calculate_reward(self):\n",
    "        # action_cost=((self.previous_action[1]-self.action[1])**2+(self.previous_action[0]-self.action[0])**2)*self.mag_cost\n",
    "        # To incorporate action_cost, we need to incorporate previous_action into decision_info\n",
    "        # self.total_time += 1\n",
    "        # In addition to rewarding the monkey for capturing the firefly, we also use different phases of rewards to teach monkey specific behaviours\n",
    "        # reward = -self.time_cost\n",
    "        reward = 0\n",
    "        # Reward shaping\n",
    "        # Phase I: reward the agent for learning to stop\n",
    "        # Always: reward the agent for capturing fireflies\n",
    "        self.num_targets = 0\n",
    "        if abs(self.sys_vel[1]) <= self.terminal_vel:\n",
    "            captured_ff_index = (self.ff_distance_all <= self.reward_boundary).nonzero().reshape(-1).tolist()\n",
    "            self.captured_ff_index = captured_ff_index\n",
    "            num_targets = len(captured_ff_index)\n",
    "            self.num_targets = num_targets\n",
    "            if num_targets > 0:  # If the monkey hs captured at least 1 ff\n",
    "                # Calculate reward\n",
    "                reward = reward + self.reward_per_ff * num_targets\n",
    "                # Replace the captured ffs with ffs of new locations\n",
    "                self.ffr[captured_ff_index] = torch.sqrt(torch.rand(num_targets)) * self.arena_radius\n",
    "                self.fftheta[captured_ff_index] = torch.rand(num_targets) * 2 * pi\n",
    "                # self.ffrt = torch.stack((self.ffr, self.fftheta), dim=1)\n",
    "                self.ffx[captured_ff_index] = torch.cos(self.fftheta[captured_ff_index]) * self.ffr[captured_ff_index]\n",
    "                self.ffy[captured_ff_index] = torch.sin(self.fftheta[captured_ff_index]) * self.ffr[captured_ff_index]\n",
    "                self.ffxy = torch.stack((self.ffx, self.ffy), dim=1)\n",
    "                self.ffx2[captured_ff_index] = self.ffx[captured_ff_index].clone()\n",
    "                self.ffy2[captured_ff_index] = self.ffy[captured_ff_index].clone()\n",
    "                self.ffxy2 = torch.stack((self.ffx2, self.ffy2), dim=1)\n",
    "                # Delete the information from self.ff_info\n",
    "                # [self.ff_info.pop(key) for key in captured_ff_index if (key in self.ff_info)]\n",
    "                # self.current_target_index = torch.tensor([999], dtype=torch.int32)\n",
    "                # self.previous_target_index = self.current_target_index\n",
    "                # Reward the firefly based on the average speed before capturing the firefly\n",
    "                ##reward = reward + sum(self.past_speeds)/len(self.past_speeds)\n",
    "                # print(round(self.time, 2), \"sys_vel: \", [round(i, 4) for i in self.sys_vel.tolist()], \"obs: \", list(np.round(self.obs, decimals = 2)), \"n_targets: \",  num_targets)\n",
    "                print(round(self.time, 2), \"sys_vel: \", [round(i, 4) for i in self.sys_vel.tolist()], \"n_targets: \",\n",
    "                      num_targets)\n",
    "                # self.update_slots = True\n",
    "                # self.stop_rewarding_speed = True\n",
    "            # elif self.has_sped_up_before == True:\n",
    "            # based on Ruiyi's formula, using the distance of the closest ff in obs\n",
    "            # reward += math.exp(-((self.ff_current[1, 0]**2)*(25/1.5)**2)/2)\n",
    "            # self.has_sped_up_before = False\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        self.time += self.dt\n",
    "        action = torch.tensor(action)\n",
    "        action[1] = action[1] / 2 + 0.5\n",
    "        self.sys_vel = action.clone()\n",
    "        self.state_step(action)\n",
    "        self.obs = self.beliefs().numpy()\n",
    "        reward = self.calculate_reward()\n",
    "        self.episode_reward += reward\n",
    "\n",
    "        if self.time >= self.episode_len * self.dt:\n",
    "            self.end_episode = True\n",
    "            # self.current_episode += 1\n",
    "            print(\"Reward for the episode: \", self.episode_reward)\n",
    "        # print(\"action: \", torch.round(action, decimals = 3), \"obs: \", np.round(self.obs, decimals = 3), \"Reward: \", round(reward, 3))\n",
    "        return self.obs, reward, self.end_episode, {}\n",
    "\n",
    "    def state_step(self, action):\n",
    "        vnoise = torch.distributions.Normal(0, torch.ones([1, 1])).sample() * self.pro_noise_std\n",
    "        wnoise = torch.distributions.Normal(0, torch.ones([1, 1])).sample() * self.pro_noise_std\n",
    "        self.dw_normed = (action[0] + wnoise)\n",
    "        self.dv_normed = (action[1] + vnoise)\n",
    "        self.dw = (action[0] + wnoise) * self.wgain * self.dt\n",
    "        self.agentheading = self.agentheading + self.dw.item()\n",
    "        self.dv = (action[1] + vnoise) * self.vgain * self.dt\n",
    "        self.dx = torch.cos(self.agentheading) * self.dv\n",
    "        self.dy = torch.sin(self.agentheading) * self.dv\n",
    "        self.agentx = self.agentx + self.dx.item()\n",
    "        self.agenty = self.agenty + self.dy.item()\n",
    "        self.agentxy = torch.cat((self.agentx, self.agenty))\n",
    "        self.agentr = vector_norm(self.agentxy)\n",
    "        self.agenttheta = torch.tensor(torch.atan2(self.agenty, self.agentx))\n",
    "\n",
    "        if self.agentr >= self.arena_radius:\n",
    "            self.agentr = 2 * self.arena_radius - self.agentr\n",
    "            self.agenttheta = self.agenttheta + pi\n",
    "            self.agentx = (self.agentr * torch.cos(self.agenttheta)).reshape(1, )\n",
    "            self.agenty = (self.agentr * torch.sin(self.agenttheta)).reshape(1, )\n",
    "            self.agentxy = torch.cat((self.agentx, self.agenty))\n",
    "            self.agentheading = self.agenttheta - pi\n",
    "        while self.agentheading >= 2 * pi:\n",
    "            self.agentheading = self.agentheading - 2 * pi\n",
    "        while self.agentheading < 0:\n",
    "            self.agentheading = self.agentheading + 2 * pi\n",
    "\n",
    "    def beliefs(self):\n",
    "\n",
    "        # Make a tensor containing the relative distance of all fireflies to the agent\n",
    "        self.ff_distance_all = vector_norm(self.ffxy - self.agentxy, dim=1)\n",
    "        # Make a tensor containing the relative (real) angle of all fireflies to the agent\n",
    "        ffradians = torch.atan2(self.ffy - self.agenty, self.ffx - self.agentx)\n",
    "        angle0 = ffradians - self.agentheading\n",
    "        angle0[angle0 > pi] = angle0[angle0 > pi] - 2 * pi\n",
    "        angle0[angle0 < -pi] = angle0[angle0 < -pi] + 2 * pi\n",
    "        # Adjust the angle based on reward boundary\n",
    "        angle1 = torch.abs(angle0) - torch.abs(torch.arcsin(torch.div(self.reward_boundary,\n",
    "                                                                      torch.clip(self.ff_distance_all,\n",
    "                                                                                 self.reward_boundary,\n",
    "                                                                                 400))))  # use torch clip to get valid arcsin input\n",
    "        angle2 = torch.clip(angle1, 0, pi)\n",
    "        ff_angle_all = torch.sign(angle0) * angle2\n",
    "        # Update the tensor containing the uncertainties of all fireflies to the agent\n",
    "        visible_ff = torch.logical_and(self.ff_distance_all < self.invisible_distance,\n",
    "                                       torch.abs(ff_angle_all) < self.invisible_angle)\n",
    "        self.visible_ff_indices0 = visible_ff.nonzero().reshape(-1)\n",
    "        for index in self.visible_ff_indices0:\n",
    "            ff = self.ff_flash[index]\n",
    "            if not torch.any(torch.logical_and(ff[:, 0] <= self.time, ff[:, 1] >= self.time)):\n",
    "                visible_ff[index] = False\n",
    "        self.visible_ff_indices = visible_ff.nonzero().reshape(-1)\n",
    "        # Update memory\n",
    "        self.ff_memory_all -= 1\n",
    "        self.ff_memory_all[self.visible_ff_indices] = self.full_memory\n",
    "        self.ff_memory_all = torch.clamp(self.ff_memory_all, 0, self.full_memory)\n",
    "        # Calculate the uncertainties that will be added to relative distance and angle based on memory\n",
    "        ff_uncertainty_all = (self.full_memory - self.ff_memory_all) * self.internal_noise_factor\n",
    "        self.ffx2 = self.ffx2 + torch.normal(torch.zeros([self.num_ff, ]), ff_uncertainty_all)\n",
    "        self.ffy2 = self.ffy2 + torch.normal(torch.zeros([self.num_ff, ]), ff_uncertainty_all)\n",
    "        self.ffx2[self.visible_ff_indices] = self.ffx[self.visible_ff_indices].clone()\n",
    "        self.ffy2[self.visible_ff_indices] = self.ffy[self.visible_ff_indices].clone()\n",
    "        self.ffxy2 = torch.stack((self.ffx2, self.ffy2), dim=1)\n",
    "        # find ffs that are in memory\n",
    "        self.ff_in_memory_indices = (self.ff_memory_all > 0).nonzero().reshape(-1)\n",
    "        # Consider the case where there are fewer than self.obs_ff fireflies that are in memory\n",
    "\n",
    "        if torch.numel(self.ff_in_memory_indices) >= self.obs_ff:\n",
    "            # Rank the ff whose \"memory\" is creater than 0 based on distance\n",
    "            sorted_indices = torch.topk(-self.ff_distance_all[self.ff_in_memory_indices], self.obs_ff).indices\n",
    "            self.topk_indices = self.ff_in_memory_indices[sorted_indices]\n",
    "            self.ffxy2_topk = self.ffxy2[self.topk_indices]\n",
    "            self.ff_distance_topk = vector_norm(self.ffxy2_topk - self.agentxy, dim=1)\n",
    "            # Calculate relative angles\n",
    "            ffradians = torch.atan2(self.ffxy2_topk[:, 1] - self.agenty, self.ffxy2_topk[:, 0] - self.agentx)\n",
    "            angle0 = ffradians - self.agentheading\n",
    "            angle0[angle0 > pi] = angle0[angle0 > pi] - 2 * pi\n",
    "            angle0[angle0 < -pi] = angle0[angle0 < -pi] + 2 * pi\n",
    "            self.ff_angle_topk_2 = angle0.clone()\n",
    "            # Calculate relative angles of all ffs based on reward boundaries\n",
    "            # Adjust the angle based on reward boundary\n",
    "            angle1 = torch.abs(angle0) - torch.abs(torch.arcsin(torch.div(self.reward_boundary,\n",
    "                                                                          torch.clip(self.ff_distance_topk,\n",
    "                                                                                     self.reward_boundary,\n",
    "                                                                                     400))))  # use torch clip to get valid arcsin input\n",
    "            angle2 = torch.clip(angle1, 0, pi)\n",
    "            ff_angle_topk_3 = torch.sign(angle0) * angle2\n",
    "            # Concatenate distance, angle, and memory\n",
    "            ff_array = torch.stack(\n",
    "                (self.ff_angle_topk_2, ff_angle_topk_3, self.ff_distance_topk, self.ff_memory_all[self.topk_indices]),\n",
    "                dim=0)\n",
    "\n",
    "\n",
    "        elif torch.numel(self.ff_in_memory_indices) == 0:\n",
    "            ff_array = torch.tensor([[0], [0], [self.invisible_distance], [0]]).repeat([1, self.obs_ff])\n",
    "            self.topk_indices = torch.tensor([])\n",
    "\n",
    "        else:\n",
    "            sorted_distance, sorted_indices = torch.sort(-self.ff_distance_all[self.ff_in_memory_indices])\n",
    "            self.topk_indices = self.ff_in_memory_indices[sorted_indices]\n",
    "            self.ffxy2_topk = self.ffxy2[self.topk_indices]\n",
    "            self.ff_distance_topk = vector_norm(self.ffxy2_topk - self.agentxy, dim=1)\n",
    "            # Calculate relative angles\n",
    "            ffradians = torch.atan2(self.ffxy2_topk[:, 1] - self.agenty, self.ffxy2_topk[:, 0] - self.agentx)\n",
    "            angle0 = ffradians - self.agentheading\n",
    "            angle0[angle0 > pi] = angle0[angle0 > pi] - 2 * pi\n",
    "            angle0[angle0 < -pi] = angle0[angle0 < -pi] + 2 * pi\n",
    "            self.ff_angle_topk_2 = angle0.clone()\n",
    "            # Calculate relative angles of all ffs based on reward boundaries\n",
    "            # Adjust the angle based on reward boundary\n",
    "            angle1 = torch.abs(angle0) - torch.abs(torch.arcsin(torch.div(self.reward_boundary,\n",
    "                                                                          torch.clip(self.ff_distance_topk,\n",
    "                                                                                     self.reward_boundary,\n",
    "                                                                                     400))))  # use torch clip to get valid arcsin input\n",
    "            angle2 = torch.clip(angle1, 0, pi)\n",
    "            ff_angle_topk_3 = torch.sign(angle0) * angle2\n",
    "            # Concatenate distance, angle, and memory\n",
    "            ff_array0 = torch.stack(\n",
    "                (self.ff_angle_topk_2, ff_angle_topk_3, self.ff_distance_topk, self.ff_memory_all[self.topk_indices]),\n",
    "                dim=0)\n",
    "            needed_ff = self.obs_ff - torch.numel(self.ff_in_memory_indices)\n",
    "            ff_array = torch.stack([ff_array0.reshape([4, -1]),\n",
    "                                    torch.tensor([[0], [0], [self.invisible_distance], [0]]).repeat([1, needed_ff])],\n",
    "                                   dim=1)\n",
    "\n",
    "        # ff_array[0:2,:] = ff_array[0:2,:]/pi\n",
    "        # ff_array[2,:] = (ff_array[2,:]/self.invisible_distance-0.5)*2\n",
    "        # ff_array[3,:] = (ff_array[3,:]/20-0.5)*2\n",
    "        self.ff_array = ff_array.clone()\n",
    "        return torch.flatten(ff_array.transpose(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bcdbe0",
   "metadata": {},
   "source": [
    "# agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e07cfbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " episode:  0\n"
     ]
    }
   ],
   "source": [
    "class CollectInformation(MultiFF):  # Note when using this wrapper, the number of steps cannot exceed one episode\n",
    "  def __init__(self):\n",
    "      super().__init__()\n",
    "      self.ff_information = np.ones([self.num_ff, 8])*(-9999)   #[index, x, y, time_start, time_captured, mx(when_captured), my(when_captured), index_in_flash]\n",
    "\n",
    "  def reset(self):\n",
    "      self.obs = super().reset()\n",
    "      self.ff_information[:,0] = np.arange(self.num_ff)\n",
    "      self.ff_information[:,7] = np.arange(self.num_ff)\n",
    "      self.ff_information[:,1] = self.ffx.numpy()\n",
    "      self.ff_information[:,2] = self.ffy.numpy()\n",
    "      self.ff_information[:,3] = 0\n",
    "      return self.obs\n",
    "\n",
    "  def calculate_reward(self):\n",
    "      reward = super().calculate_reward()\n",
    "      if self.num_targets > 0:\n",
    "        for index in self.captured_ff_index:\n",
    "          overall_index = int(self.ff_information[:,0][np.where(self.ff_information[:,-1]==index)[0][-1]])\n",
    "          self.ff_information[overall_index, 4] = self.time\n",
    "          self.ff_information[overall_index, 5] = self.agentx.item()\n",
    "          self.ff_information[overall_index, 6] = self.agenty.item()\n",
    "        self.new_ff_info = np.ones([self.num_targets, 8])*(-9999)\n",
    "        self.new_ff_info[:,0] = np.arange(len(self.ff_information), len(self.ff_information)+self.num_targets)\n",
    "        self.new_ff_info[:,7] = np.array(self.captured_ff_index)\n",
    "        self.new_ff_info[:,1] = self.ffx[self.captured_ff_index].numpy()\n",
    "        self.new_ff_info[:,2] = self.ffy[self.captured_ff_index].numpy()\n",
    "        self.new_ff_info[:,3] = self.time\n",
    "        self.ff_information = np.concatenate([self.ff_information, self.new_ff_info], axis = 0)\n",
    "      return(reward)\n",
    "\n",
    "\n",
    "env = CollectInformation()\n",
    "env = Monitor(env, log_dir)\n",
    "env.reset()\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=20000, log_dir=log_dir)\n",
    "\n",
    "sac_model = SAC(\"MlpPolicy\",\n",
    "            env,\n",
    "            buffer_size=int(1e6),\n",
    "            batch_size=1024,\n",
    "            device='auto',\n",
    "            verbose=False,\n",
    "            train_freq=100,\n",
    "            learning_starts = int(10),\n",
    "            target_update_interval=20,\n",
    "            learning_rate=1e-2,\n",
    "            gamma=0.9999,\n",
    "            policy_kwargs=dict(activation_fn=nn.ReLU, net_arch=[64, 64])\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ef428b",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7224d2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " episode:  1\n"
     ]
    }
   ],
   "source": [
    "timesteps = 50000000\n",
    "sac_model.learn(total_timesteps=int(timesteps), callback=callback)\n",
    "plot_results([log_dir], timesteps, results_plotter.X_TIMESTEPS, \"Multiff\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f9cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5bc4a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fcdb0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ff_venv",
   "language": "python",
   "name": "ff_venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
