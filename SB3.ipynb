{"cells":[{"cell_type":"markdown","id":"899a1d5c","metadata":{"toc":true,"id":"899a1d5c"},"source":["<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n","<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#import-packages\" data-toc-modified-id=\"import-packages-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>import packages</a></span></li><li><span><a href=\"#env\" data-toc-modified-id=\"env-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>env</a></span><ul class=\"toc-item\"><li><span><a href=\"#c16.2\" data-toc-modified-id=\"c16.2-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>c16.2</a></span></li></ul></li><li><span><a href=\"#agent\" data-toc-modified-id=\"agent-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>agent</a></span></li><li><span><a href=\"#train\" data-toc-modified-id=\"train-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>train</a></span></li></ul></div>"]},{"cell_type":"markdown","source":["# Set up"],"metadata":{"id":"J1gAADXTIEVw"},"id":"J1gAADXTIEVw"},{"cell_type":"markdown","source":["## install packages"],"metadata":{"id":"1t_MmbnQxz3f"},"id":"1t_MmbnQxz3f"},{"cell_type":"code","source":["from google.colab import drive # import drive from google colab\n","drive.mount(\"/content/drive\") \n","!pip install neo\n","!pip install matplotlib_scalebar\n","!pip install ffmpeg\n","!pip install stable_baselines3\n","!pip install optuna"],"metadata":{"id":"NQsJKP7xpsJi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676235811236,"user_tz":360,"elapsed":43024,"user":{"displayName":"Cici Du","userId":"17701548280142155870"}},"outputId":"17938a7b-a06e-4405-ed8b-7756bdeb7c05"},"id":"NQsJKP7xpsJi","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting neo\n","  Downloading neo-0.11.1.tar.gz (3.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from neo) (23.0)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from neo) (1.21.6)\n","Collecting quantities>=0.12.1\n","  Downloading quantities-0.14.0-py3-none-any.whl (87 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.8/87.8 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: neo\n","  Building wheel for neo (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for neo: filename=neo-0.11.1-py3-none-any.whl size=579963 sha256=b87a5b526d20c8c6f05cd54d1ad3ea975a8cc8c56bfe72fdb4da815354fc3911\n","  Stored in directory: /root/.cache/pip/wheels/6a/63/0c/ec05bf584fd57600854a6832201659728466464abd19afebb4\n","Successfully built neo\n","Installing collected packages: quantities, neo\n","Successfully installed neo-0.11.1 quantities-0.14.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting matplotlib_scalebar\n","  Downloading matplotlib_scalebar-0.8.1-py2.py3-none-any.whl (17 kB)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from matplotlib_scalebar) (3.2.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->matplotlib_scalebar) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->matplotlib_scalebar) (1.4.4)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->matplotlib_scalebar) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->matplotlib_scalebar) (2.8.2)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.8/dist-packages (from matplotlib->matplotlib_scalebar) (1.21.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->matplotlib_scalebar) (1.15.0)\n","Installing collected packages: matplotlib_scalebar\n","Successfully installed matplotlib_scalebar-0.8.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ffmpeg\n","  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: ffmpeg\n","  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6084 sha256=8e5a3eeb465cf5206693700588d25614673bb49825ab8d8d9cf1ac5afd499997\n","  Stored in directory: /root/.cache/pip/wheels/30/33/46/5ab7eca55b9490dddbf3441c68a29535996270ef1ce8b9b6d7\n","Successfully built ffmpeg\n","Installing collected packages: ffmpeg\n","Successfully installed ffmpeg-1.4\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting stable_baselines3\n","  Downloading stable_baselines3-1.7.0-py3-none-any.whl (171 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.8/171.8 KB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gym==0.21\n","  Downloading gym-0.21.0.tar.gz (1.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.8/dist-packages (from stable_baselines3) (1.13.1+cu116)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from stable_baselines3) (1.21.6)\n","Collecting importlib-metadata~=4.13\n","  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from stable_baselines3) (1.3.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from stable_baselines3) (3.2.2)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from stable_baselines3) (2.2.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata~=4.13->stable_baselines3) (3.12.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.11->stable_baselines3) (4.4.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable_baselines3) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable_baselines3) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable_baselines3) (0.11.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->stable_baselines3) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->stable_baselines3) (1.15.0)\n","Building wheels for collected packages: gym\n","  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616821 sha256=f0b2076d1d903c50c49c232da25d5f796b15d2b19fb5ecaeca67470e0da87d89\n","  Stored in directory: /root/.cache/pip/wheels/27/6d/b3/a3a6e10704795c9b9000f1ab2dc480dfe7bed42f5972806e73\n","Successfully built gym\n","Installing collected packages: importlib-metadata, gym, stable_baselines3\n","  Attempting uninstall: importlib-metadata\n","    Found existing installation: importlib-metadata 6.0.0\n","    Uninstalling importlib-metadata-6.0.0:\n","      Successfully uninstalled importlib-metadata-6.0.0\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.25.2\n","    Uninstalling gym-0.25.2:\n","      Successfully uninstalled gym-0.25.2\n","Successfully installed gym-0.21.0 importlib-metadata-4.13.0 stable_baselines3-1.7.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting optuna\n","  Downloading optuna-3.1.0-py3-none-any.whl (365 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.3/365.3 KB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from optuna) (1.21.6)\n","Collecting colorlog\n","  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (1.4.46)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from optuna) (6.0)\n","Collecting cmaes>=0.9.1\n","  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n","Collecting alembic>=1.5.0\n","  Downloading alembic-1.9.3-py3-none-any.whl (210 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.6/210.6 KB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (23.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from optuna) (4.64.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from alembic>=1.5.0->optuna) (4.13.0)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from alembic>=1.5.0->optuna) (5.10.2)\n","Collecting Mako\n","  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->alembic>=1.5.0->optuna) (3.12.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n","Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n","Successfully installed Mako-1.2.4 alembic-1.9.3 cmaes-0.9.1 colorlog-6.7.0 optuna-3.1.0\n"]}]},{"cell_type":"markdown","id":"9904b001","metadata":{"id":"9904b001"},"source":["## import packages"]},{"cell_type":"code","execution_count":2,"id":"5fdd43fd","metadata":{"id":"5fdd43fd","outputId":"7a7efc2e-23e8-4ec7-b2d6-da50a0620631","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676235822605,"user_tz":360,"elapsed":11373,"user":{"displayName":"Cici Du","userId":"17701548280142155870"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/ff_repo/Multifirefly-Project\n"]}],"source":["%cd /content/drive/MyDrive/ff_repo/Multifirefly-Project\n","from model_storing_path import *\n","from config import *\n","from RL.env import *\n","from functions.SB3_functions import *\n","from functions.interpret_neural_network import interpret_neural_network_func\n","from functions.find_patterns import *\n","from functions.collect_agent_data import collect_agent_data_func\n","from functions.animation_func import *\n","\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import optuna\n","import matplotlib\n","import matplotlib.animation as animation\n","from matplotlib import rc, cm\n","from stable_baselines3 import SAC\n","from stable_baselines3.common.monitor import Monitor\n","from stable_baselines3.common import results_plotter\n","from stable_baselines3.common.results_plotter import plot_results\n","from optuna.pruners import MedianPruner\n","from optuna.samplers import TPESampler\n","from IPython.display import HTML\n","from functools import partial\n","\n","plt.rcParams[\"animation.html\"] = \"html5\"\n","os.environ['KMP_DUPLICATE_LIB_OK']='True'\n","rc('animation', html='jshtml')\n","matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n","matplotlib.rcParams['animation.embed_limit'] = 2**128\n","torch.set_printoptions(sci_mode=False)\n","pd.set_option('display.float_format', lambda x: '%.5f' % x)\n","np.set_printoptions(suppress=True)"]},{"cell_type":"markdown","source":["# Env"],"metadata":{"id":"M0ih1oFjSKzZ"},"id":"M0ih1oFjSKzZ"},{"cell_type":"markdown","source":["## regular"],"metadata":{"id":"nbldTm3-SNsz"},"id":"nbldTm3-SNsz"},{"cell_type":"code","source":["env = MultiFF()\n","env = Monitor(env, log_dir)"],"metadata":{"id":"cr-5hLTISSzn","executionInfo":{"status":"ok","timestamp":1676235823091,"user_tz":360,"elapsed":487,"user":{"displayName":"Cici Du","userId":"17701548280142155870"}}},"id":"cr-5hLTISSzn","execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## one-ff"],"metadata":{"id":"BZErTDlmSOp_"},"id":"BZErTDlmSOp_"},{"cell_type":"code","source":["env = MultiFF(num_obs_ff=1)\n","env = Monitor(env, log_dir)"],"metadata":{"id":"47efcmZzSSKv","executionInfo":{"status":"ok","timestamp":1676235823225,"user_tz":360,"elapsed":135,"user":{"displayName":"Cici Du","userId":"17701548280142155870"}}},"id":"47efcmZzSSKv","execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Agent"],"metadata":{"id":"9G46vEanSLwC"},"id":"9G46vEanSLwC"},{"cell_type":"markdown","id":"b0bcdbe0","metadata":{"id":"b0bcdbe0"},"source":["## make agent"]},{"cell_type":"code","source":["log_dir = \"RL/SB3_stored_models/SB3_Feb_12\"\n","os.makedirs(log_dir, exist_ok=True)\n","\n","\n","env = MultiFF()\n","env = Monitor(env, log_dir)\n","\n","\n","# For direct training\n","sac_model = SAC(\"MlpPolicy\", \n","            env,\n","            gamma=0.995,\n","            learning_rate=0.0015,\n","            batch_size=1024,\n","            target_update_interval=50,\n","            buffer_size=1000000,\n","            learning_starts=10000,\n","            train_freq=10,\n","            ent_coef=0.00083,\n","            policy_kwargs=dict(activation_fn=nn.Tanh, net_arch=[128, 128])\n","                )"],"metadata":{"id":"qLYkVSa9h6_l","executionInfo":{"status":"ok","timestamp":1676235823333,"user_tz":360,"elapsed":113,"user":{"displayName":"Cici Du","userId":"17701548280142155870"}}},"id":"qLYkVSa9h6_l","execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## load agent (optional)"],"metadata":{"id":"7nrqpCXeIKNC"},"id":"7nrqpCXeIKNC"},{"cell_type":"code","source":["path = os.path.join(log_dir, 'best_model.zip')\n","path2 = os.path.join(log_dir, 'buffer.pkl')\n","sac_model = sac_model.load(path,env=env) \n","sac_model.load_replay_buffer(path2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":371},"id":"nkZCiiRaIMBM","executionInfo":{"status":"error","timestamp":1676235823638,"user_tz":360,"elapsed":306,"user":{"displayName":"Cici Du","userId":"17701548280142155870"}},"outputId":"91d27cd6-cded-4a5f-f2c6-135f6c95b826"},"id":"nkZCiiRaIMBM","execution_count":6,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-0a160d2b99be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'best_model.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'buffer.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msac_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msac_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msac_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_replay_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/base_class.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0mget_system_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         data, params, pytorch_variables = load_from_zip_file(\n\u001b[0m\u001b[1;32m    660\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/save_util.py\u001b[0m in \u001b[0;36mload_from_zip_file\u001b[0;34m(load_path, load_data, custom_objects, device, verbose, print_system_info)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0mdict\u001b[0m \u001b[0mof\u001b[0m \u001b[0mpytorch\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     \"\"\"\n\u001b[0;32m--> 390\u001b[0;31m     \u001b[0mload_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;31m# set device to cpu if cuda is not available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.8/functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    873\u001b[0m                             '1 positional argument')\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m     \u001b[0mfuncname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__name__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'singledispatch function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/save_util.py\u001b[0m in \u001b[0;36mopen_path_str\u001b[0;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.8/functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    873\u001b[0m                             '1 positional argument')\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m     \u001b[0mfuncname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__name__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'singledispatch function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/save_util.py\u001b[0m in \u001b[0;36mopen_path_pathlib\u001b[0;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;31m# if reading failed with FileNotFoundError, calls open_path_pathlib with suffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.8/functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    873\u001b[0m                             '1 positional argument')\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m     \u001b[0mfuncname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__name__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'singledispatch function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/save_util.py\u001b[0m in \u001b[0;36mopen_path_pathlib\u001b[0;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/save_util.py\u001b[0m in \u001b[0;36mopen_path_pathlib\u001b[0;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.8/pathlib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         return io.open(self, mode, buffering, encoding, errors, newline,\n\u001b[0m\u001b[1;32m   1223\u001b[0m                        opener=self._opener)\n\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.8/pathlib.py\u001b[0m in \u001b[0;36m_opener\u001b[0;34m(self, name, flags, mode)\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0o666\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m         \u001b[0;31m# A stub for the opener argument to built-in open()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_raw_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0o777\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'RL/SB3_stored_models/SB3_Feb_12/best_model.zip.zip'"]}]},{"cell_type":"markdown","id":"c0ef428b","metadata":{"id":"c0ef428b"},"source":["# Train"]},{"cell_type":"code","execution_count":7,"id":"7224d2a6","metadata":{"id":"7224d2a6","executionInfo":{"status":"error","timestamp":1676236938596,"user_tz":360,"elapsed":123503,"user":{"displayName":"Cici Du","userId":"17701548280142155870"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"b6703e74-fa20-4f4e-f0c4-092c61e54bf6"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/content/drive/MyDrive/ff_repo/Multifirefly-Project/RL/env.py:629: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  ff_flashing_durations = torch.tensor(ff_flash[index])\n"]},{"output_type":"stream","name":"stdout","text":["188.0 sys_vel:  [0.1673, 0.0062] n_targets:  1\n","Reward for the episode:  100\n","\n"," episode:  1\n","196.25 sys_vel:  [0.2583, 0.0014] n_targets:  1\n","Reward for the episode:  100\n","\n"," episode:  2\n","Reward for the episode:  0\n","\n"," episode:  3\n","57.25 sys_vel:  [0.9064, 0.0008] n_targets:  2\n","Reward for the episode:  200\n","\n"," episode:  4\n","Reward for the episode:  0\n","\n"," episode:  5\n","Reward for the episode:  0\n","\n"," episode:  6\n","115.0 sys_vel:  [-0.2915, 0.0069] n_targets:  1\n","Reward for the episode:  100\n","\n"," episode:  7\n","Reward for the episode:  0\n","\n"," episode:  8\n","Reward for the episode:  0\n","\n"," episode:  9\n","140.0 sys_vel:  [0.1292, 0.0088] n_targets:  1\n","213.25 sys_vel:  [0.2117, 0.0042] n_targets:  1\n","Reward for the episode:  200\n","\n"," episode:  10\n","55.0 sys_vel:  [0.7598, 0.0043] n_targets:  1\n","Reward for the episode:  100\n","\n"," episode:  11\n","82.5 sys_vel:  [0.4219, 0.0044] n_targets:  1\n","117.0 sys_vel:  [0.9291, 0.0094] n_targets:  1\n","134.75 sys_vel:  [-0.8049, 0.0042] n_targets:  1\n","Reward for the episode:  300\n","\n"," episode:  12\n","120.5 sys_vel:  [0.7723, 0.0059] n_targets:  1\n","Reward for the episode:  100\n","\n"," episode:  13\n","Reward for the episode:  0\n","\n"," episode:  14\n","36.0 sys_vel:  [0.9815, 0.0008] n_targets:  1\n","Reward for the episode:  100\n","\n"," episode:  15\n","1.25 sys_vel:  [0.9981, 0.0028] n_targets:  1\n","Reward for the episode:  100\n","\n"," episode:  16\n","5.5 sys_vel:  [0.9997, 0.0003] n_targets:  1\n","Reward for the episode:  100\n","\n"," episode:  17\n","0.5 sys_vel:  [0.9469, 0.0062] n_targets:  1\n","179.75 sys_vel:  [0.9712, 0.0093] n_targets:  1\n","Reward for the episode:  200\n","\n"," episode:  18\n","3.5 sys_vel:  [0.9977, 0.0018] n_targets:  1\n","194.0 sys_vel:  [0.9987, 0.0002] n_targets:  1\n","Reward for the episode:  200\n","\n"," episode:  19\n","87.5 sys_vel:  [0.9976, 0.0007] n_targets:  1\n","122.5 sys_vel:  [0.9976, 0.0017] n_targets:  1\n"," \n","Num timesteps: 20000\n","Best mean reward: -inf - Last mean reward per episode: 100.00\n","Saving new best model to RL/SB3_stored_models/SB3_Feb_12/best_model\n","233.5 sys_vel:  [0.998, 0.0016] n_targets:  1\n","Reward for the episode:  300\n","\n"," episode:  20\n","4.75 sys_vel:  [0.9849, 0.0066] n_targets:  1\n","29.25 sys_vel:  [0.9998, 0.0] n_targets:  1\n","44.0 sys_vel:  [0.9929, 0.0011] n_targets:  1\n","85.75 sys_vel:  [0.9662, 0.0064] n_targets:  1\n","174.25 sys_vel:  [0.9891, 0.0002] n_targets:  1\n","236.75 sys_vel:  [0.9526, 0.0026] n_targets:  1\n","237.25 sys_vel:  [0.9958, 0.0014] n_targets:  1\n","239.75 sys_vel:  [0.9962, 0.0019] n_targets:  1\n","244.25 sys_vel:  [0.3348, 0.0069] n_targets:  1\n","Reward for the episode:  900\n","\n"," episode:  21\n","8.75 sys_vel:  [0.9783, 0.0089] n_targets:  1\n","71.25 sys_vel:  [0.9999, 0.0001] n_targets:  1\n","89.5 sys_vel:  [0.9662, 0.0094] n_targets:  1\n","90.75 sys_vel:  [0.9876, 0.0005] n_targets:  1\n","93.75 sys_vel:  [0.5925, 0.0084] n_targets:  1\n","103.75 sys_vel:  [0.9448, 0.0067] n_targets:  2\n","109.75 sys_vel:  [0.9369, 0.0063] n_targets:  1\n","221.75 sys_vel:  [0.9946, 0.0002] n_targets:  1\n","243.0 sys_vel:  [0.9539, 0.003] n_targets:  1\n","253.0 sys_vel:  [0.9966, 0.0071] n_targets:  1\n","Reward for the episode:  1100\n","\n"," episode:  22\n","23.75 sys_vel:  [0.9999, 0.0] n_targets:  1\n","177.25 sys_vel:  [0.9829, 0.0007] n_targets:  1\n","Reward for the episode:  200\n","\n"," episode:  23\n","0.25 sys_vel:  [0.9981, 0.0003] n_targets:  1\n","Reward for the episode:  100\n","\n"," episode:  24\n","2.25 sys_vel:  [0.943, 0.0079] n_targets:  1\n","Reward for the episode:  100\n","\n"," episode:  25\n","Reward for the episode:  0\n","\n"," episode:  26\n","Reward for the episode:  0\n","\n"," episode:  27\n","54.25 sys_vel:  [0.8799, 0.0097] n_targets:  1\n","85.5 sys_vel:  [0.8533, 0.0025] n_targets:  1\n","100.5 sys_vel:  [0.7645, 0.0075] n_targets:  1\n","252.0 sys_vel:  [0.9655, 0.0043] n_targets:  1\n","Reward for the episode:  400\n","\n"," episode:  28\n","75.5 sys_vel:  [0.9977, 0.0] n_targets:  1\n","102.75 sys_vel:  [0.9998, 0.0] n_targets:  1\n","163.0 sys_vel:  [0.7145, 0.0051] n_targets:  1\n","176.25 sys_vel:  [0.9754, 0.0] n_targets:  1\n","181.0 sys_vel:  [0.9905, 0.0] n_targets:  2\n","207.5 sys_vel:  [0.9593, 0.0002] n_targets:  1\n","208.5 sys_vel:  [0.8376, 0.0013] n_targets:  1\n","239.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","241.25 sys_vel:  [0.9987, 0.0008] n_targets:  1\n","Reward for the episode:  1000\n","\n"," episode:  29\n","3.75 sys_vel:  [0.7849, 0.0083] n_targets:  1\n","11.25 sys_vel:  [0.9997, 0.0] n_targets:  1\n","13.75 sys_vel:  [0.9999, 0.0] n_targets:  1\n","35.5 sys_vel:  [0.1634, 0.0023] n_targets:  1\n","67.25 sys_vel:  [0.9991, 0.0] n_targets:  1\n","75.25 sys_vel:  [0.6078, 0.0095] n_targets:  1\n","77.75 sys_vel:  [0.9949, 0.0001] n_targets:  1\n","81.5 sys_vel:  [0.9913, 0.0] n_targets:  2\n","108.25 sys_vel:  [0.9998, 0.0] n_targets:  1\n","173.25 sys_vel:  [0.0706, 0.0051] n_targets:  1\n","222.0 sys_vel:  [0.9995, 0.0] n_targets:  1\n","242.25 sys_vel:  [0.9927, 0.0004] n_targets:  1\n","Reward for the episode:  1300\n","\n"," episode:  30\n","60.75 sys_vel:  [0.9987, 0.0] n_targets:  1\n","74.5 sys_vel:  [0.9983, 0.0] n_targets:  1\n","83.0 sys_vel:  [0.9911, 0.0001] n_targets:  1\n","91.5 sys_vel:  [0.9973, 0.0001] n_targets:  1\n","94.75 sys_vel:  [0.9994, 0.0] n_targets:  2\n","135.5 sys_vel:  [0.8569, 0.0031] n_targets:  1\n","173.25 sys_vel:  [0.8364, 0.0003] n_targets:  1\n","177.25 sys_vel:  [0.9946, 0.0004] n_targets:  1\n","220.25 sys_vel:  [0.9437, 0.002] n_targets:  1\n","238.5 sys_vel:  [0.7246, 0.0058] n_targets:  2\n","239.25 sys_vel:  [0.969, 0.0024] n_targets:  1\n","Reward for the episode:  1300\n","\n"," episode:  31\n","15.75 sys_vel:  [0.9999, 0.0] n_targets:  1\n","16.75 sys_vel:  [0.9999, 0.0] n_targets:  1\n","41.75 sys_vel:  [0.8001, 0.0005] n_targets:  1\n","43.25 sys_vel:  [0.719, 0.0008] n_targets:  1\n","50.75 sys_vel:  [0.9988, 0.0] n_targets:  1\n","145.25 sys_vel:  [0.8208, 0.0018] n_targets:  1\n","228.5 sys_vel:  [0.9785, 0.0006] n_targets:  1\n","Reward for the episode:  700\n","\n"," episode:  32\n","1.0 sys_vel:  [0.9583, 0.0075] n_targets:  1\n","138.75 sys_vel:  [0.9992, 0.0001] n_targets:  1\n","180.0 sys_vel:  [0.9988, 0.0002] n_targets:  1\n","216.25 sys_vel:  [0.997, 0.0001] n_targets:  1\n","Reward for the episode:  400\n","\n"," episode:  33\n","14.5 sys_vel:  [0.9999, 0.0] n_targets:  1\n","62.25 sys_vel:  [0.969, 0.0068] n_targets:  1\n","68.25 sys_vel:  [0.9942, 0.0007] n_targets:  1\n","Reward for the episode:  300\n","\n"," episode:  34\n","50.75 sys_vel:  [0.9997, 0.0] n_targets:  1\n","70.5 sys_vel:  [0.9947, 0.0] n_targets:  1\n","115.25 sys_vel:  [0.9988, 0.0] n_targets:  1\n","126.75 sys_vel:  [0.999, 0.0] n_targets:  1\n","190.75 sys_vel:  [0.9988, 0.0] n_targets:  1\n","Reward for the episode:  500\n","\n"," episode:  35\n","10.75 sys_vel:  [0.6039, 0.0045] n_targets:  2\n","41.5 sys_vel:  [0.9988, 0.0] n_targets:  1\n","169.75 sys_vel:  [0.6319, 0.0079] n_targets:  1\n","Reward for the episode:  400\n","\n"," episode:  36\n","26.25 sys_vel:  [0.9998, 0.0001] n_targets:  1\n","108.75 sys_vel:  [0.999, 0.0001] n_targets:  1\n","117.25 sys_vel:  [0.9446, 0.0049] n_targets:  1\n","169.75 sys_vel:  [0.9988, 0.0] n_targets:  1\n","225.0 sys_vel:  [0.4162, 0.0041] n_targets:  1\n","239.5 sys_vel:  [0.0266, 0.0051] n_targets:  1\n","241.0 sys_vel:  [0.9999, 0.0001] n_targets:  1\n","Reward for the episode:  700\n","\n"," episode:  37\n","37.75 sys_vel:  [0.9998, 0.0] n_targets:  1\n","94.5 sys_vel:  [0.9983, 0.0003] n_targets:  1\n","114.25 sys_vel:  [0.9999, 0.0] n_targets:  1\n","156.75 sys_vel:  [0.9065, 0.0067] n_targets:  1\n","Reward for the episode:  400\n","\n"," episode:  38\n","0.75 sys_vel:  [0.9995, 0.0] n_targets:  1\n","9.0 sys_vel:  [0.9956, 0.0002] n_targets:  1\n","66.0 sys_vel:  [0.9998, 0.0] n_targets:  1\n","114.5 sys_vel:  [0.9838, 0.0007] n_targets:  1\n","134.0 sys_vel:  [0.9557, 0.0005] n_targets:  1\n","143.0 sys_vel:  [0.8287, 0.0059] n_targets:  1\n","242.25 sys_vel:  [0.9251, 0.0013] n_targets:  1\n","Reward for the episode:  700\n","\n"," episode:  39\n","9.5 sys_vel:  [0.9923, 0.0026] n_targets:  1\n","10.75 sys_vel:  [0.9927, 0.0008] n_targets:  1\n"," \n","Num timesteps: 40000\n","Best mean reward: 100.00 - Last mean reward per episode: 325.64\n","Saving new best model to RL/SB3_stored_models/SB3_Feb_12/best_model\n","113.5 sys_vel:  [0.9983, 0.0001] n_targets:  1\n","141.75 sys_vel:  [0.2463, 0.0005] n_targets:  1\n","149.75 sys_vel:  [0.2346, 0.0016] n_targets:  1\n","175.0 sys_vel:  [0.9937, 0.0005] n_targets:  1\n","178.5 sys_vel:  [0.9999, 0.0] n_targets:  1\n","180.75 sys_vel:  [0.9999, 0.0] n_targets:  1\n","183.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","196.25 sys_vel:  [0.9999, 0.0001] n_targets:  1\n","236.0 sys_vel:  [0.9049, 0.0085] n_targets:  1\n","Reward for the episode:  1100\n","\n"," episode:  40\n","12.5 sys_vel:  [0.8553, 0.0031] n_targets:  1\n","25.0 sys_vel:  [0.9723, 0.0038] n_targets:  1\n","36.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","39.5 sys_vel:  [0.8739, 0.0067] n_targets:  1\n","44.5 sys_vel:  [0.9809, 0.0044] n_targets:  1\n","45.5 sys_vel:  [0.948, 0.0041] n_targets:  1\n","46.5 sys_vel:  [0.8755, 0.0034] n_targets:  1\n","57.75 sys_vel:  [0.5557, 0.0024] n_targets:  1\n","71.0 sys_vel:  [0.6234, 0.0041] n_targets:  1\n","94.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","110.5 sys_vel:  [-0.6769, 0.0048] n_targets:  1\n","137.5 sys_vel:  [0.9996, 0.0] n_targets:  1\n","141.75 sys_vel:  [0.9993, 0.0] n_targets:  1\n","144.75 sys_vel:  [0.9998, 0.0001] n_targets:  1\n","184.0 sys_vel:  [0.2813, 0.0099] n_targets:  1\n","186.25 sys_vel:  [0.9855, 0.0017] n_targets:  1\n","187.5 sys_vel:  [0.9986, 0.0001] n_targets:  1\n","228.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1900\n","\n"," episode:  41\n","10.25 sys_vel:  [1.0, 0.0001] n_targets:  1\n","16.75 sys_vel:  [0.9999, 0.0] n_targets:  1\n","23.0 sys_vel:  [0.6419, 0.0013] n_targets:  1\n","57.5 sys_vel:  [0.9998, 0.0002] n_targets:  1\n","59.75 sys_vel:  [0.9998, 0.0007] n_targets:  1\n","63.75 sys_vel:  [0.9984, 0.0014] n_targets:  1\n","64.5 sys_vel:  [1.0, 0.0002] n_targets:  1\n","80.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","86.0 sys_vel:  [0.9998, 0.0] n_targets:  1\n","96.25 sys_vel:  [0.9999, 0.0] n_targets:  1\n","99.25 sys_vel:  [0.9919, 0.0003] n_targets:  1\n","114.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","119.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","125.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","132.0 sys_vel:  [0.9999, 0.0] n_targets:  1\n","141.5 sys_vel:  [0.9998, 0.0] n_targets:  1\n","143.0 sys_vel:  [0.9998, 0.0] n_targets:  1\n","161.25 sys_vel:  [0.991, 0.0007] n_targets:  1\n","179.25 sys_vel:  [0.9998, 0.0] n_targets:  1\n","180.0 sys_vel:  [0.9999, 0.0] n_targets:  1\n","182.5 sys_vel:  [0.9998, 0.0] n_targets:  1\n","187.0 sys_vel:  [0.9872, 0.001] n_targets:  1\n","216.75 sys_vel:  [0.9993, 0.0002] n_targets:  1\n","224.5 sys_vel:  [0.9995, 0.0] n_targets:  1\n","233.0 sys_vel:  [0.9993, 0.0] n_targets:  1\n","233.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2600\n","\n"," episode:  42\n","35.0 sys_vel:  [0.9967, 0.0001] n_targets:  1\n","40.0 sys_vel:  [0.9864, 0.0006] n_targets:  1\n","98.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","101.75 sys_vel:  [0.9844, 0.0009] n_targets:  2\n","106.5 sys_vel:  [0.9999, 0.0] n_targets:  2\n","139.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","146.5 sys_vel:  [0.9855, 0.0046] n_targets:  1\n","147.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","154.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","164.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","175.25 sys_vel:  [0.9999, 0.0] n_targets:  1\n","222.0 sys_vel:  [0.9999, 0.0] n_targets:  1\n","Reward for the episode:  1400\n","\n"," episode:  43\n","4.75 sys_vel:  [0.993, 0.0033] n_targets:  1\n","44.25 sys_vel:  [0.9998, 0.0] n_targets:  1\n","65.5 sys_vel:  [0.9999, 0.0] n_targets:  2\n","67.75 sys_vel:  [0.1798, 0.0025] n_targets:  1\n","72.0 sys_vel:  [0.9999, 0.0] n_targets:  1\n","83.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","98.5 sys_vel:  [0.9944, 0.0001] n_targets:  1\n","100.5 sys_vel:  [0.9858, 0.0] n_targets:  1\n","125.0 sys_vel:  [0.9231, 0.0011] n_targets:  1\n","149.25 sys_vel:  [0.9993, 0.0] n_targets:  1\n","157.75 sys_vel:  [0.9999, 0.0] n_targets:  1\n","163.25 sys_vel:  [0.9898, 0.0008] n_targets:  1\n","Reward for the episode:  1300\n","\n"," episode:  44\n","18.25 sys_vel:  [0.5114, 0.0025] n_targets:  1\n","43.5 sys_vel:  [0.9951, 0.0] n_targets:  1\n","45.25 sys_vel:  [0.9948, 0.0002] n_targets:  1\n","88.25 sys_vel:  [0.9535, 0.0029] n_targets:  1\n","125.5 sys_vel:  [-0.3979, 0.0064] n_targets:  1\n","135.25 sys_vel:  [-0.735, 0.0] n_targets:  1\n","138.25 sys_vel:  [0.9996, 0.0001] n_targets:  1\n","145.75 sys_vel:  [0.9995, 0.0] n_targets:  1\n","192.5 sys_vel:  [0.9999, 0.0] n_targets:  1\n","193.75 sys_vel:  [0.9995, 0.0] n_targets:  1\n","203.75 sys_vel:  [0.9986, 0.0] n_targets:  2\n","212.25 sys_vel:  [0.9984, 0.0] n_targets:  1\n","217.25 sys_vel:  [0.9966, 0.0002] n_targets:  1\n","221.25 sys_vel:  [0.9959, 0.0004] n_targets:  1\n","Reward for the episode:  1500\n","\n"," episode:  45\n","178.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","206.25 sys_vel:  [0.9999, 0.0] n_targets:  1\n","207.25 sys_vel:  [0.9998, 0.0] n_targets:  1\n","208.5 sys_vel:  [0.9998, 0.0] n_targets:  1\n","223.25 sys_vel:  [0.9984, 0.0001] n_targets:  1\n","236.75 sys_vel:  [0.9867, 0.0001] n_targets:  1\n","238.5 sys_vel:  [0.9946, 0.0001] n_targets:  2\n","246.25 sys_vel:  [0.9808, 0.0004] n_targets:  1\n","247.0 sys_vel:  [0.9853, 0.0002] n_targets:  1\n","251.0 sys_vel:  [0.9973, 0.0] n_targets:  1\n","Reward for the episode:  1100\n","\n"," episode:  46\n","8.75 sys_vel:  [0.9841, 0.0077] n_targets:  1\n","10.75 sys_vel:  [0.9944, 0.0003] n_targets:  1\n","27.75 sys_vel:  [0.9998, 0.0] n_targets:  1\n","49.5 sys_vel:  [0.9984, 0.0] n_targets:  1\n","108.75 sys_vel:  [0.9997, 0.0] n_targets:  1\n","119.5 sys_vel:  [0.9996, 0.0] n_targets:  1\n","121.5 sys_vel:  [0.9989, 0.0] n_targets:  1\n","132.0 sys_vel:  [0.9821, 0.0008] n_targets:  1\n","153.75 sys_vel:  [0.9999, 0.0] n_targets:  1\n","160.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","197.25 sys_vel:  [0.9999, 0.0] n_targets:  1\n","243.0 sys_vel:  [0.9999, 0.0] n_targets:  1\n","246.25 sys_vel:  [0.9999, 0.0] n_targets:  1\n","247.5 sys_vel:  [0.999, 0.0] n_targets:  1\n","253.25 sys_vel:  [0.9999, 0.0] n_targets:  1\n","Reward for the episode:  1500\n","\n"," episode:  47\n","1.75 sys_vel:  [0.9997, 0.0] n_targets:  1\n","14.75 sys_vel:  [0.9998, 0.0] n_targets:  2\n","31.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","33.25 sys_vel:  [0.9999, 0.0] n_targets:  1\n","34.75 sys_vel:  [0.9997, 0.0003] n_targets:  1\n","37.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","49.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","59.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","67.5 sys_vel:  [0.9999, 0.0] n_targets:  1\n","70.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","91.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","102.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","108.5 sys_vel:  [0.9989, 0.0] n_targets:  1\n","118.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","124.5 sys_vel:  [0.9993, 0.0] n_targets:  1\n","129.5 sys_vel:  [0.9834, 0.0014] n_targets:  1\n","145.5 sys_vel:  [0.9998, 0.0] n_targets:  1\n","149.75 sys_vel:  [0.9999, 0.0] n_targets:  1\n","155.5 sys_vel:  [0.9999, 0.0] n_targets:  1\n","172.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","177.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","180.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","192.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","199.5 sys_vel:  [0.9999, 0.0] n_targets:  1\n","200.75 sys_vel:  [0.9999, 0.0] n_targets:  1\n","205.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","237.75 sys_vel:  [0.9998, 0.0001] n_targets:  1\n","244.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","248.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  3000\n","\n"," episode:  48\n","0.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","8.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","17.25 sys_vel:  [0.9999, 0.0] n_targets:  1\n","24.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","33.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","42.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","49.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","86.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","88.75 sys_vel:  [0.9996, 0.0013] n_targets:  2\n","98.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","101.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","104.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","107.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","115.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","120.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","127.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","136.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","139.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","150.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","152.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","185.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","199.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","201.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","205.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","231.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","244.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","247.5 sys_vel:  [-0.9813, 0.0018] n_targets:  1\n","Reward for the episode:  3000\n","\n"," episode:  49\n","13.75 sys_vel:  [0.9998, 0.0002] n_targets:  1\n","22.25 sys_vel:  [0.9999, 0.0002] n_targets:  1\n","58.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","62.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","79.75 sys_vel:  [1.0, 0.0001] n_targets:  1\n","85.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","88.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","100.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","104.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","120.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","121.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","137.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","165.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","193.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","196.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","200.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","215.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","227.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","246.0 sys_vel:  [0.9996, 0.0006] n_targets:  1\n","Reward for the episode:  1900\n","\n"," episode:  50\n","4.25 sys_vel:  [0.9328, 0.001] n_targets:  1\n","10.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","30.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","31.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","33.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","34.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","40.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","58.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","59.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","76.75 sys_vel:  [0.9999, 0.0013] n_targets:  1\n","88.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","102.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","107.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","137.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","151.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","153.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","173.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","187.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","194.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","206.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","208.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","216.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","221.0 sys_vel:  [1.0, 0.0004] n_targets:  1\n","255.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2800\n","\n"," episode:  51\n","10.5 sys_vel:  [1.0, 0.0004] n_targets:  1\n","19.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","26.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","47.75 sys_vel:  [1.0, 0.0001] n_targets:  2\n","54.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","72.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","73.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","73.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","87.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","91.75 sys_vel:  [1.0, 0.0003] n_targets:  1\n","102.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","110.5 sys_vel:  [1.0, 0.0005] n_targets:  1\n","112.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","131.0 sys_vel:  [0.9999, 0.0] n_targets:  2\n","145.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","146.0 sys_vel:  [0.9998, 0.0] n_targets:  1\n","157.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","171.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","179.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","199.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","200.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","210.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","213.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","229.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","255.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","256.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2900\n","\n"," episode:  52\n","5.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","13.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","23.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","28.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","33.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","42.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","44.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","69.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","83.25 sys_vel:  [1.0, 0.0005] n_targets:  1\n","94.0 sys_vel:  [1.0, 0.0021] n_targets:  1\n","96.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","120.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","141.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","144.5 sys_vel:  [1.0, 0.0001] n_targets:  1\n","154.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","176.25 sys_vel:  [1.0, 0.0004] n_targets:  1\n","188.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","191.0 sys_vel:  [1.0, 0.0001] n_targets:  1\n","207.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","223.75 sys_vel:  [1.0, 0.0058] n_targets:  1\n","242.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2200\n","\n"," episode:  53\n","1.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","29.75 sys_vel:  [1.0, 0.0006] n_targets:  1\n","58.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","65.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","92.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","93.25 sys_vel:  [0.9999, 0.0] n_targets:  1\n","121.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","124.75 sys_vel:  [0.9998, 0.0018] n_targets:  1\n","135.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","157.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","162.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","169.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","232.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","235.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","244.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","248.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1600\n","\n"," episode:  54\n","16.25 sys_vel:  [1.0, 0.0004] n_targets:  1\n","18.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","19.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","31.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","39.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","74.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","97.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","116.75 sys_vel:  [1.0, 0.0001] n_targets:  1\n","121.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","148.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","159.75 sys_vel:  [1.0, 0.0089] n_targets:  1\n","191.75 sys_vel:  [1.0, 0.0001] n_targets:  1\n","218.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","228.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","235.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","249.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","251.75 sys_vel:  [1.0, 0.0002] n_targets:  1\n","255.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1900\n","\n"," episode:  55\n","10.75 sys_vel:  [1.0, 0.0001] n_targets:  1\n","12.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","19.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","34.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","48.0 sys_vel:  [1.0, 0.0012] n_targets:  1\n","50.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","54.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","70.5 sys_vel:  [1.0, 0.0031] n_targets:  1\n","89.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","188.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","191.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","193.5 sys_vel:  [1.0, 0.0002] n_targets:  1\n","204.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","219.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","224.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","228.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","235.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","255.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1900\n","\n"," episode:  56\n","18.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","35.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","42.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","45.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","58.0 sys_vel:  [1.0, 0.0012] n_targets:  1\n","63.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","75.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","98.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","116.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","124.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","150.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","157.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","170.5 sys_vel:  [1.0, 0.0023] n_targets:  2\n","193.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","198.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","204.75 sys_vel:  [1.0, 0.0] n_targets:  2\n","212.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","228.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","233.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","239.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2500\n","\n"," episode:  57\n","9.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","11.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","21.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","30.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","47.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","50.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","53.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","58.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","63.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","65.25 sys_vel:  [1.0, 0.0031] n_targets:  1\n","74.0 sys_vel:  [0.7588, 0.0081] n_targets:  1\n","77.25 sys_vel:  [1.0, 0.0001] n_targets:  1\n","85.25 sys_vel:  [1.0, 0.0002] n_targets:  1\n","89.25 sys_vel:  [0.4542, 0.0073] n_targets:  1\n","93.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","101.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","106.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","113.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","118.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","120.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","136.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","136.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","138.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","166.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","168.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","181.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","196.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","219.75 sys_vel:  [1.0, 0.0] n_targets:  3\n","226.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","245.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","248.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  3600\n","\n"," episode:  58\n","13.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","21.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","24.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","33.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","42.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","88.25 sys_vel:  [1.0, 0.0001] n_targets:  1\n","94.75 sys_vel:  [1.0, 0.0001] n_targets:  1\n","101.75 sys_vel:  [1.0, 0.0001] n_targets:  2\n","103.5 sys_vel:  [1.0, 0.0001] n_targets:  1\n","108.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","113.25 sys_vel:  [1.0, 0.0011] n_targets:  1\n","116.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","120.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","141.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","142.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","149.75 sys_vel:  [1.0, 0.0] n_targets:  1\n"," \n","Num timesteps: 60000\n","Best mean reward: 325.64 - Last mean reward per episode: 903.45\n","Saving new best model to RL/SB3_stored_models/SB3_Feb_12/best_model\n","154.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","164.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","182.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","187.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","202.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","208.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","211.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","212.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","241.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2600\n","\n"," episode:  59\n","4.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","36.25 sys_vel:  [1.0, 0.0013] n_targets:  1\n","46.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","65.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","81.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","93.5 sys_vel:  [1.0, 0.0003] n_targets:  1\n","95.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","96.25 sys_vel:  [1.0, 0.0001] n_targets:  1\n","98.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","105.75 sys_vel:  [1.0, 0.0] n_targets:  2\n","110.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","132.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","150.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","161.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","162.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","168.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","169.25 sys_vel:  [1.0, 0.0023] n_targets:  1\n","171.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","180.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","182.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","190.0 sys_vel:  [1.0, 0.0001] n_targets:  1\n","192.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","209.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","211.5 sys_vel:  [1.0, 0.0001] n_targets:  1\n","223.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","238.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","252.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2900\n","\n"," episode:  60\n","18.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","22.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","32.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","35.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","75.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","82.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","88.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","95.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","104.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","108.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","123.5 sys_vel:  [1.0, 0.0071] n_targets:  1\n","125.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","126.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","127.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","149.75 sys_vel:  [1.0, 0.0002] n_targets:  1\n","165.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","174.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","217.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","234.25 sys_vel:  [1.0, 0.0002] n_targets:  1\n","235.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","Reward for the episode:  2300\n","\n"," episode:  61\n","4.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","8.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","16.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","32.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","79.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","81.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","87.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","96.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","108.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","125.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","127.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","130.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","135.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","148.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","151.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","156.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","164.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","184.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","203.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","216.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","220.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","231.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","234.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","243.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","249.25 sys_vel:  [1.0, 0.0031] n_targets:  1\n","252.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2800\n","\n"," episode:  62\n","25.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","28.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","37.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","47.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","55.25 sys_vel:  [1.0, 0.0059] n_targets:  1\n","59.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","66.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","69.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","82.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","93.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","101.25 sys_vel:  [1.0, 0.0001] n_targets:  1\n","153.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","194.25 sys_vel:  [1.0, 0.0054] n_targets:  1\n","223.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","237.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","243.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","251.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2000\n","\n"," episode:  63\n","1.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","16.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","24.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","28.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","29.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","30.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","32.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","53.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","62.0 sys_vel:  [1.0, 0.0003] n_targets:  1\n","68.75 sys_vel:  [1.0, 0.0001] n_targets:  1\n","70.0 sys_vel:  [1.0, 0.0003] n_targets:  1\n","72.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","79.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","86.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","155.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","157.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","173.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","184.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","190.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","205.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","213.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","226.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","255.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2400\n","\n"," episode:  64\n","16.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","25.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","34.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","52.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","61.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","63.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","146.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","147.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","153.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","160.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","165.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","169.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","170.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","173.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","174.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","180.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","197.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","206.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","208.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","215.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2200\n","\n"," episode:  65\n","6.75 sys_vel:  [1.0, 0.0] n_targets:  2\n","29.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","34.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","37.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","57.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","58.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","119.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","144.75 sys_vel:  [1.0, 0.0] n_targets:  2\n","162.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","168.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1200\n","\n"," episode:  66\n","3.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","21.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","81.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","95.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","103.0 sys_vel:  [1.0, 0.0019] n_targets:  1\n","118.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","124.75 sys_vel:  [1.0, 0.001] n_targets:  2\n","149.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","180.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","219.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1100\n","\n"," episode:  67\n","0.75 sys_vel:  [1.0, 0.0] n_targets:  2\n","23.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","40.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","46.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","51.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","72.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","75.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","102.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","115.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","118.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","170.75 sys_vel:  [1.0, 0.0] n_targets:  2\n","188.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1400\n","\n"," episode:  68\n","31.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","35.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","36.5 sys_vel:  [1.0, 0.0001] n_targets:  1\n","37.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","57.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","96.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","103.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","116.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","119.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","128.75 sys_vel:  [1.0, 0.0] n_targets:  2\n","139.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","155.0 sys_vel:  [1.0, 0.0001] n_targets:  1\n","159.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","174.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","241.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","244.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","252.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1800\n","\n"," episode:  69\n","6.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","13.5 sys_vel:  [1.0, 0.0002] n_targets:  1\n","14.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","27.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","49.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","51.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","67.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","75.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","83.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","139.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","152.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","152.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","153.75 sys_vel:  [1.0, 0.0] n_targets:  2\n","158.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","166.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","170.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","175.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","195.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","201.0 sys_vel:  [1.0, 0.0] n_targets:  3\n","224.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","229.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","234.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","237.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","250.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","255.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2800\n","\n"," episode:  70\n","4.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","27.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","38.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","45.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","48.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","160.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","166.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","176.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","247.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","253.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1100\n","\n"," episode:  71\n","9.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","75.75 sys_vel:  [1.0, 0.0] n_targets:  2\n","77.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","83.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","87.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","93.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","95.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","136.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","163.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1000\n","\n"," episode:  72\n","2.75 sys_vel:  [1.0, 0.0027] n_targets:  1\n","12.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","14.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","24.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","27.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","34.5 sys_vel:  [1.0, 0.0008] n_targets:  1\n","39.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","61.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","62.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","71.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","73.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","75.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","81.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","87.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","92.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","106.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","114.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","114.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","117.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","119.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","121.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","146.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","180.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","192.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","195.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","196.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2800\n","\n"," episode:  73\n","5.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","13.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","35.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","86.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","109.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","112.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","154.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","189.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","241.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  900\n","\n"," episode:  74\n","2.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","4.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","8.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","23.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","30.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","112.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","123.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  700\n","\n"," episode:  75\n","0.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","1.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","15.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","33.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","44.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","58.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","74.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","102.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","142.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","184.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","186.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","199.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1300\n","\n"," episode:  76\n","11.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","12.0 sys_vel:  [0.9926, 0.0034] n_targets:  1\n","27.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","50.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","51.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","122.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","127.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","132.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","137.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","150.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","163.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","183.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","207.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","214.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","237.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","252.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","253.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","255.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1800\n","\n"," episode:  77\n","0.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","2.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","7.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","26.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","36.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","45.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","61.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","70.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","73.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","76.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","80.0 sys_vel:  [0.9965, 0.0009] n_targets:  1\n","81.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","91.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","94.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","112.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","113.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","118.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","121.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","137.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","141.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","143.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","189.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","196.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","203.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","214.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","218.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","222.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","234.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","242.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","253.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  3000\n","\n"," episode:  78\n","22.0 sys_vel:  [1.0, 0.0] n_targets:  1\n"," \n","Num timesteps: 80000\n","Best mean reward: 903.45 - Last mean reward per episode: 1160.26\n","Saving new best model to RL/SB3_stored_models/SB3_Feb_12/best_model\n","34.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","51.25 sys_vel:  [1.0, 0.0017] n_targets:  1\n","73.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","121.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","127.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","177.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","239.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","251.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  900\n","\n"," episode:  79\n","17.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","29.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","30.75 sys_vel:  [1.0, 0.0002] n_targets:  1\n","36.0 sys_vel:  [0.9401, 0.006] n_targets:  2\n","42.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","60.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","98.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","106.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","111.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","119.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","132.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","138.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","142.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","143.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","155.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","177.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","189.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","200.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","219.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","225.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","Reward for the episode:  2300\n","\n"," episode:  80\n","2.5 sys_vel:  [0.9998, 0.0001] n_targets:  1\n","32.0 sys_vel:  [0.9961, 0.0021] n_targets:  1\n","47.0 sys_vel:  [0.9833, 0.0043] n_targets:  1\n","89.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","91.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","96.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","99.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","113.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","119.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","127.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","154.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","157.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","166.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","190.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","192.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","207.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","222.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1700\n","\n"," episode:  81\n","5.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","21.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","24.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","27.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","54.5 sys_vel:  [0.9892, 0.0018] n_targets:  1\n","62.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","121.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","127.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","146.0 sys_vel:  [1.0, 0.0001] n_targets:  1\n","154.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","169.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","182.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","190.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","216.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","244.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1700\n","\n"," episode:  82\n","4.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","7.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","18.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","36.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","59.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","64.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","93.75 sys_vel:  [0.9997, 0.0004] n_targets:  1\n","96.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","105.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","112.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","113.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","159.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","176.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","197.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","204.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","218.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","227.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","230.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","231.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","238.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","243.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","244.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","248.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","253.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2500\n","\n"," episode:  83\n","7.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","11.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","11.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","54.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","88.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","91.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","143.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","165.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","206.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","225.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1000\n","\n"," episode:  84\n","1.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","42.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","49.5 sys_vel:  [0.9981, 0.0079] n_targets:  1\n","69.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","74.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","77.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","82.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","85.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","96.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","106.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","114.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","141.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","171.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1300\n","\n"," episode:  85\n","2.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","5.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","7.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","17.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","20.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","34.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","45.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","47.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","55.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","57.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","64.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","87.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","99.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","152.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","161.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","163.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","165.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","229.25 sys_vel:  [0.9998, 0.0007] n_targets:  1\n","230.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1900\n","\n"," episode:  86\n","10.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","11.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","12.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","34.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","35.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","37.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","67.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","104.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  800\n","\n"," episode:  87\n","2.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","4.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","7.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","26.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","33.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","62.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","134.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","161.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","205.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","215.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1000\n","\n"," episode:  88\n","2.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","6.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","12.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","20.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","26.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","33.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","45.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","65.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","84.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","91.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","233.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","234.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","242.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","249.75 sys_vel:  [1.0, 0.0] n_targets:  2\n","253.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1600\n","\n"," episode:  89\n","6.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","16.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","19.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","23.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","30.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","51.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","62.5 sys_vel:  [0.9994, 0.0001] n_targets:  1\n","63.0 sys_vel:  [0.9982, 0.0001] n_targets:  2\n","66.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","72.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","75.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","120.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","134.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","135.5 sys_vel:  [0.9984, 0.0001] n_targets:  1\n","144.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","146.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1800\n","\n"," episode:  90\n","20.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","41.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","54.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","56.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","59.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","121.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  600\n","\n"," episode:  91\n","2.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","5.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","14.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","36.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","45.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","45.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","47.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","49.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","53.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","73.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","76.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","84.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","101.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","178.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","181.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","194.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","202.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","207.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","209.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","209.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","221.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","230.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","238.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","247.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","249.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","250.75 sys_vel:  [1.0, 0.0] n_targets:  2\n","254.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  3100\n","\n"," episode:  92\n","0.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","2.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","4.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","11.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","18.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","31.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","85.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","93.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","102.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","111.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","115.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","126.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","127.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1300\n","\n"," episode:  93\n","3.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","7.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","17.75 sys_vel:  [1.0, 0.0] n_targets:  3\n","20.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","40.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","110.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","147.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","155.75 sys_vel:  [1.0, 0.0] n_targets:  2\n","157.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","167.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","169.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","171.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","245.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1700\n","\n"," episode:  94\n","9.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","11.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","15.75 sys_vel:  [1.0, 0.0] n_targets:  2\n","22.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","39.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","82.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","115.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","125.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1000\n","\n"," episode:  95\n","14.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","44.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","47.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","57.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","96.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","111.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","120.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","146.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","154.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","159.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","163.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","208.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","216.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","219.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","222.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","238.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","253.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1800\n","\n"," episode:  96\n","2.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","3.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","5.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","17.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","52.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","56.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","63.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","68.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","80.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","82.25 sys_vel:  [1.0, 0.0006] n_targets:  1\n","93.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","120.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","127.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","135.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","153.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","158.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","214.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","218.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","219.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","246.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2000\n","\n"," episode:  97\n","11.75 sys_vel:  [0.9999, 0.0001] n_targets:  1\n","12.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","35.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","43.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","49.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","57.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","60.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","94.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","100.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","107.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","109.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","110.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","112.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","142.75 sys_vel:  [1.0, 0.0] n_targets:  1\n"," \n","Num timesteps: 100000\n","Best mean reward: 1160.26 - Last mean reward per episode: 1242.27\n","Saving new best model to RL/SB3_stored_models/SB3_Feb_12/best_model\n","191.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","194.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","200.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","205.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","224.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","224.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","239.0 sys_vel:  [0.9998, 0.0] n_targets:  1\n","249.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","251.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","254.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2500\n","\n"," episode:  98\n","7.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","9.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","12.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","16.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","24.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","72.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","82.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","105.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","124.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","164.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","168.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","179.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","206.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","214.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","221.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","253.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","254.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1900\n","\n"," episode:  99\n","2.25 sys_vel:  [1.0, 0.0001] n_targets:  1\n","3.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","3.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","8.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","10.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","21.75 sys_vel:  [0.9997, 0.0017] n_targets:  1\n","28.5 sys_vel:  [0.9994, 0.0003] n_targets:  2\n","37.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","63.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","75.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","98.0 sys_vel:  [0.9999, 0.0] n_targets:  1\n","100.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","132.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","143.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","145.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","155.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","182.75 sys_vel:  [0.9988, 0.0] n_targets:  1\n","197.5 sys_vel:  [0.9913, 0.0002] n_targets:  1\n","199.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","202.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","227.25 sys_vel:  [0.9992, 0.0] n_targets:  1\n","229.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","243.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","250.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","252.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2700\n","\n"," episode:  100\n","27.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","30.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","31.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","36.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","46.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","51.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  600\n","\n"," episode:  101\n","0.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","3.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","4.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","9.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","10.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","43.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","109.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","112.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","145.5 sys_vel:  [0.9999, 0.0] n_targets:  1\n","146.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","150.0 sys_vel:  [1.0, 0.0] n_targets:  3\n","154.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","160.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","165.75 sys_vel:  [0.9999, 0.0] n_targets:  1\n","208.75 sys_vel:  [0.9995, 0.0001] n_targets:  1\n","247.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","249.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2100\n","\n"," episode:  102\n","13.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","27.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","33.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","42.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","55.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","57.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","71.5 sys_vel:  [0.9985, 0.0051] n_targets:  1\n","72.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","73.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","76.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","77.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","79.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","89.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","90.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","91.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","104.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","107.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","135.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","145.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","153.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","154.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","165.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","172.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","175.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","176.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","189.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","192.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","195.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","197.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","201.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","212.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","215.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","220.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  3600\n","\n"," episode:  103\n","3.25 sys_vel:  [1.0, 0.0001] n_targets:  1\n","8.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","12.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","15.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","40.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","44.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","69.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","76.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","82.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","91.0 sys_vel:  [0.9985, 0.0005] n_targets:  1\n","123.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","129.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","147.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","149.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","155.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","159.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","168.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","196.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","198.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1900\n","\n"," episode:  104\n","12.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","29.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","30.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","37.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","40.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","49.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","52.75 sys_vel:  [0.9998, 0.0] n_targets:  1\n","55.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","68.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","72.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","84.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","88.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","95.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","107.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","116.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","116.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","128.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","129.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","135.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","142.0 sys_vel:  [0.9986, 0.0001] n_targets:  1\n","152.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","158.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","159.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","166.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","170.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","177.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","186.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","190.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","209.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","211.0 sys_vel:  [1.0, 0.0] n_targets:  3\n","214.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","224.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","240.75 sys_vel:  [0.9873, 0.0012] n_targets:  1\n","245.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","246.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  3800\n","\n"," episode:  105\n","23.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","36.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","40.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","95.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","100.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","102.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","106.5 sys_vel:  [1.0, 0.0002] n_targets:  1\n","112.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","121.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","125.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","157.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1100\n","\n"," episode:  106\n","15.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","27.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","45.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","55.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","58.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","61.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","63.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","105.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","112.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","115.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","120.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","143.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","149.0 sys_vel:  [0.9999, 0.0] n_targets:  1\n","159.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","188.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1500\n","\n"," episode:  107\n","8.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","48.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","91.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","129.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","148.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","155.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","182.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","189.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","196.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","200.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","208.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","233.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","237.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1500\n","\n"," episode:  108\n","35.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","47.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","53.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","55.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","58.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","71.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","78.5 sys_vel:  [0.9999, 0.0] n_targets:  1\n","79.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","96.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","103.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","108.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","112.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","120.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","122.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","156.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","174.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","176.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2000\n","\n"," episode:  109\n","1.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","18.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","19.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","27.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","52.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","69.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","89.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","96.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","102.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","118.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","123.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","136.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","144.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","151.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","152.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","175.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","180.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","181.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","196.75 sys_vel:  [0.6998, 0.0052] n_targets:  1\n","211.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","219.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","224.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","230.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","238.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","251.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2600\n","\n"," episode:  110\n","1.5 sys_vel:  [0.9992, 0.0001] n_targets:  1\n","3.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","7.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","15.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","21.5 sys_vel:  [0.9876, 0.0002] n_targets:  1\n","27.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","48.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","56.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","59.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","63.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","75.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","81.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","85.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","86.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","91.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","111.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","117.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","120.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","136.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","141.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","168.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","170.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","196.0 sys_vel:  [0.9934, 0.0007] n_targets:  1\n","197.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","198.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","204.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","207.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","218.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","224.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  3000\n","\n"," episode:  111\n","1.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","4.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","12.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","21.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","30.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","37.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","57.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","60.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","69.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","79.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","102.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","107.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","109.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","127.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","133.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","137.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","148.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","151.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","175.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","178.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","215.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","229.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","243.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2400\n","\n"," episode:  112\n","8.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","32.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","43.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","54.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","60.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","115.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","125.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","127.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","137.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","140.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","141.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","145.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","151.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","162.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","170.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","172.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","193.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","212.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","220.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","223.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","229.25 sys_vel:  [0.9996, 0.0001] n_targets:  1\n","230.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","231.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","233.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","241.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","246.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","247.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","250.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2900\n","\n"," episode:  113\n","20.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","24.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","29.25 sys_vel:  [0.9994, 0.0] n_targets:  1\n","37.75 sys_vel:  [1.0, 0.0] n_targets:  2\n","44.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","73.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","91.75 sys_vel:  [1.0, 0.0] n_targets:  2\n","94.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","98.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","112.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","127.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","130.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","135.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","164.0 sys_vel:  [0.9991, 0.0001] n_targets:  1\n","175.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","195.75 sys_vel:  [1.0, 0.0] n_targets:  2\n","210.0 sys_vel:  [-0.717, 0.0095] n_targets:  1\n","215.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","218.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","226.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","232.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","243.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","245.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","253.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  3000\n","\n"," episode:  114\n","10.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","12.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","16.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","16.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","17.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","23.0 sys_vel:  [1.0, 0.0] n_targets:  3\n","32.75 sys_vel:  [0.9999, 0.0] n_targets:  1\n","36.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","49.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","53.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","54.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","125.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","138.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","144.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","159.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","161.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","171.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","187.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","191.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","206.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","212.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","216.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","216.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","226.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","245.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","254.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  3000\n","\n"," episode:  115\n","0.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","8.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","44.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","67.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","77.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","81.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","122.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","124.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","135.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","159.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","167.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","190.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","199.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","209.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","214.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","223.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","239.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1900\n","\n"," episode:  116\n","5.5 sys_vel:  [0.9644, 0.0077] n_targets:  1\n","6.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","8.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","28.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","29.75 sys_vel:  [0.9998, 0.0] n_targets:  1\n","34.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","36.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","42.0 sys_vel:  [0.9998, 0.0] n_targets:  1\n","43.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","55.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","67.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","68.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","83.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","87.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","89.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","90.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","91.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","114.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","132.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","134.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","135.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","140.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","150.25 sys_vel:  [0.9994, 0.0004] n_targets:  2\n","155.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","190.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","194.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","198.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","199.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","205.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","208.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","211.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","229.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","230.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","243.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","245.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","251.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  4000\n","\n"," episode:  117\n","0.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","4.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","11.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","19.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","29.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","42.25 sys_vel:  [1.0, 0.0] n_targets:  1\n"," \n","Num timesteps: 120000\n","Best mean reward: 1242.27 - Last mean reward per episode: 1670.00\n","Saving new best model to RL/SB3_stored_models/SB3_Feb_12/best_model\n","61.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","66.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","73.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","104.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","118.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","147.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","156.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","180.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","187.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1800\n","\n"," episode:  118\n","11.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","22.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","25.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","33.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","46.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","51.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","54.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","56.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","72.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","74.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","88.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","113.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","115.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","164.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","165.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","175.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","184.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","187.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","254.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2000\n","\n"," episode:  119\n","3.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","8.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","26.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","27.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","28.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","44.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","45.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","56.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","59.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","78.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","80.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","93.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","102.75 sys_vel:  [0.9996, 0.0] n_targets:  1\n","125.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","128.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","137.75 sys_vel:  [1.0, 0.0] n_targets:  2\n","139.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","149.25 sys_vel:  [1.0, 0.0] n_targets:  3\n","150.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","165.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","166.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","167.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","171.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","189.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","198.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","200.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","206.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","236.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","238.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","240.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","243.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  3700\n","\n"," episode:  120\n","8.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","18.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","28.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","36.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","44.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","52.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","52.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","55.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","56.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","70.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","88.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","105.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","114.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","119.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","126.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","147.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","156.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","158.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","168.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","182.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","200.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","202.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","226.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","254.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2500\n","\n"," episode:  121\n","8.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","27.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","31.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","48.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","62.25 sys_vel:  [0.9999, 0.0001] n_targets:  1\n","68.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","76.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","81.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","82.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","94.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","98.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","101.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","121.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","132.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","141.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","148.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","149.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","151.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","160.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","161.75 sys_vel:  [1.0, 0.0] n_targets:  2\n","175.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","189.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","191.75 sys_vel:  [1.0, 0.0] n_targets:  2\n","198.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","204.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","223.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","236.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","248.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","254.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","Reward for the episode:  3500\n","\n"," episode:  122\n","14.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","34.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","48.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","54.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","55.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","60.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","122.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","128.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","138.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","150.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","165.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","165.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","167.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","170.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","177.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","191.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","195.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","199.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","240.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","241.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2000\n","\n"," episode:  123\n","17.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","28.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","29.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","47.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","48.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","60.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","63.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","79.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","80.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","82.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","86.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","90.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","94.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","95.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","108.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","109.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","120.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","121.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","136.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","141.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","143.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","151.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","161.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","166.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","182.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","205.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","222.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","225.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","235.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  3100\n","\n"," episode:  124\n","24.75 sys_vel:  [0.9997, 0.0] n_targets:  1\n","48.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","51.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","90.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","114.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","118.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","134.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","145.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","155.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","162.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","177.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","224.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","225.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","246.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","249.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","252.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1600\n","\n"," episode:  125\n","2.75 sys_vel:  [1.0, 0.0] n_targets:  2\n","14.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","35.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","47.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","66.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","69.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","75.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","82.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","119.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","122.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","123.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","139.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","143.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","150.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","175.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","177.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","183.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","194.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","196.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","203.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","210.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","219.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","221.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","231.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","233.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","240.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","249.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","253.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  3000\n","\n"," episode:  126\n","16.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","47.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","52.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","76.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","91.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","104.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","105.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","116.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","127.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","134.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","140.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","143.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","153.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","183.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","202.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1500\n","\n"," episode:  127\n","1.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","3.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","16.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","23.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","26.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","28.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","38.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","40.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","41.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","46.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","47.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","52.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","76.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","128.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","138.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","140.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","141.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","151.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","173.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","174.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","175.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","177.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","179.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","183.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","200.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","215.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","255.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  3000\n","\n"," episode:  128\n","88.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","91.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","159.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","164.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","170.5 sys_vel:  [0.9986, 0.001] n_targets:  1\n","171.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","184.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","191.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","215.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","219.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","221.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1200\n","\n"," episode:  129\n","3.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","59.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","61.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","91.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","121.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","170.75 sys_vel:  [0.9995, 0.0003] n_targets:  1\n","184.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","199.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","205.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","207.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","218.5 sys_vel:  [1.0, 0.0] n_targets:  2\n","240.25 sys_vel:  [0.0419, 0.0043] n_targets:  1\n","242.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","249.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","251.75 sys_vel:  [1.0, 0.0] n_targets:  2\n","252.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","253.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1900\n","\n"," episode:  130\n","3.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","8.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","27.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","29.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","49.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","51.0 sys_vel:  [0.9997, 0.0] n_targets:  1\n","68.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","72.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","74.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","87.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","90.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","92.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","106.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","117.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","124.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","130.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","136.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","161.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","172.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","173.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","176.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","177.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","184.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","200.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","223.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","236.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","245.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","250.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","252.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2900\n","\n"," episode:  131\n","1.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","3.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","5.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","7.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","17.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","26.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","29.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","33.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","38.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","55.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","72.25 sys_vel:  [0.1857, 0.0038] n_targets:  1\n","83.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","89.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","98.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","111.75 sys_vel:  [1.0, 0.0] n_targets:  2\n","122.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","142.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","144.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","147.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","154.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","170.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","172.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","175.75 sys_vel:  [1.0, 0.0] n_targets:  2\n","212.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","214.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","225.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","238.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","252.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","255.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  3400\n","\n"," episode:  132\n","2.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","3.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","4.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","26.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","54.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","65.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","70.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","71.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","80.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","95.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","99.25 sys_vel:  [0.9997, 0.0001] n_targets:  1\n","119.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","121.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","122.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","123.75 sys_vel:  [1.0, 0.0] n_targets:  2\n","127.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","144.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","146.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","162.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","188.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","192.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","196.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","213.75 sys_vel:  [1.0, 0.0] n_targets:  2\n","214.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","229.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","254.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","255.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  3000\n","\n"," episode:  133\n","58.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","64.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","64.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","67.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","74.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","75.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","89.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","100.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","127.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","129.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","131.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","141.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","147.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","157.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","165.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","172.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","174.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","175.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","193.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","208.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","212.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","214.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","215.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","238.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","251.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2600\n","\n"," episode:  134\n","12.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","26.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","27.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","42.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","47.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","50.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","54.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","65.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","77.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","142.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","149.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","155.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","162.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","164.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","167.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","170.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","192.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","199.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","201.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","202.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","206.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","241.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","244.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2400\n","\n"," episode:  135\n","18.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","34.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","35.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","36.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","62.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","66.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","196.0 sys_vel:  [-0.9095, 0.0019] n_targets:  1\n","198.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","212.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","218.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1000\n","\n"," episode:  136\n","18.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","39.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","46.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","50.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","54.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","55.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","73.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","74.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","77.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","82.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","87.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","98.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","100.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","120.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","138.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","176.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","182.25 sys_vel:  [1.0, 0.0] n_targets:  1\n"," \n","Num timesteps: 140000\n","Best mean reward: 1670.00 - Last mean reward per episode: 2037.00\n","Saving new best model to RL/SB3_stored_models/SB3_Feb_12/best_model\n","185.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","194.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","202.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","241.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","248.5 sys_vel:  [-0.9792, 0.0013] n_targets:  1\n","Reward for the episode:  2300\n","\n"," episode:  137\n","4.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","5.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","11.25 sys_vel:  [-0.7014, 0.0093] n_targets:  1\n","24.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","25.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","25.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","27.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","51.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","66.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","90.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","109.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","116.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","132.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","133.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","153.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","156.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","170.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","186.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","194.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","194.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","208.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","220.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","242.0 sys_vel:  [1.0, 0.0] n_targets:  2\n","250.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  2500\n","\n"," episode:  138\n","4.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","7.25 sys_vel:  [1.0, 0.0] n_targets:  2\n","9.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","27.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","45.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","49.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","66.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","69.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","73.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","84.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","86.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","96.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","134.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","164.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","192.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","206.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","209.5 sys_vel:  [1.0, 0.0] n_targets:  1\n","215.25 sys_vel:  [1.0, 0.0] n_targets:  1\n","Reward for the episode:  1900\n","\n"," episode:  139\n","1.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","3.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","11.0 sys_vel:  [1.0, 0.0] n_targets:  1\n","19.75 sys_vel:  [1.0, 0.0] n_targets:  1\n","22.0 sys_vel:  [1.0, 0.0] n_targets:  1\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-5ddb22804605>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveOnBestTrainingRewardCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtimesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50000000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msac_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplot_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_plotter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_TIMESTEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Multiff\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/sac/sac.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    297\u001b[0m     ) -> SelfSAC:\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    300\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0;31m# Special case when the user passes `gradient_steps=0`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgradient_steps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgradient_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_training_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/sac/sac.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;31m# Get current Q-values estimates for each critic network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;31m# using action from the replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mcurrent_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;31m# Compute critic loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, obs, actions)\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_extractor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0mqvalue_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqvalue_input\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq_net\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_networks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mq1_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_extractor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0mqvalue_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqvalue_input\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq_net\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_networks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mq1_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mSiLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["callback = SaveOnBestTrainingRewardCallback(check_freq=20000, log_dir=log_dir)\n","timesteps = 50000000\n","sac_model.learn(total_timesteps=int(timesteps), callback=callback)\n","plot_results([log_dir], timesteps, results_plotter.X_TIMESTEPS, \"Multiff\")\n","plt.show()"]},{"cell_type":"markdown","source":["# Test"],"metadata":{"id":"r6j1e2uxq5OG"},"id":"r6j1e2uxq5OG"},{"cell_type":"code","source":["obs = env.reset()\n","cum_rewards = 0\n","for step in range(1000):\n","    action, _ = sac_model.predict(obs, deterministic=True)\n","    obs, reward, done, info = env.step(action)\n","    cum_rewards += reward\n","    if done:\n","        obs = env.reset()\n","    # print(step, ffxy_visible[-1])\n","print(cum_rewards)"],"metadata":{"id":"k9-tdbAHq5_U","executionInfo":{"status":"aborted","timestamp":1676235823638,"user_tz":360,"elapsed":17,"user":{"displayName":"Cici Du","userId":"17701548280142155870"}}},"id":"k9-tdbAHq5_U","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Hyperparameter tuning"],"metadata":{"id":"1Inbx37xwycW"},"id":"1Inbx37xwycW"},{"cell_type":"markdown","source":["## parameters to sample from"],"metadata":{"id":"hfHYXONdz7qJ"},"id":"hfHYXONdz7qJ"},{"cell_type":"code","source":["def sample_sac_params(trial):\n","    \"\"\"\n","    Sampler for SAC hyperparams.\n","\n","\n","    Parameters\n","    ----------\n","    trial: (optuna.trial)\n","\n","    Return: (dict)\n","    \"\"\"\n","\n","\n","    gamma = 1.0 - trial.suggest_float(\"gamma\", 0.0001, 0.1, log=True)\n","    learning_rate = trial.suggest_float(\"lr\", 1e-5, 1, log=True)\n","    tau = trial.suggest_float(\"tau\", 1e-6, 1, log=True)\n","    #batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128, 256, 512, 1024])\n","    target_update_interval = trial.suggest_categorical('target_update_interval', [5, 10, 20, 40, 60, 100, 200])\n","    #buffer_size = trial.suggest_categorical('buffer_size', [int(1e5), int(1e6)]) # This actually doesn't matter much here because of limited timesteps\n","    learning_starts = trial.suggest_categorical('learning_starts', [5000, 10000, 15000])\n","    train_freq = trial.suggest_categorical('train_freq', [1, 10, 100, 300])\n","    ## gradient_steps takes too much time\n","    # gradient_steps = trial.suggest_categorical('gradient_steps', [1, 100, 300])\n","    gradient_steps = train_freq\n","    ent_coef = trial.suggest_float(\"ent_coef\", 0.00000001, 0.1, log=True)\n","    net_arch = trial.suggest_categorical('net_arch', [\"small\", \"medium\", \"big\"])\n","    activation_fn = trial.suggest_categorical(\"activation_fn\", [\"tanh\"])\n","\n","    net_arch = {\n","        'small': [100, 100],\n","        'medium': [128, 128],\n","        'big': [200, 200],\n","    }[net_arch]\n","\n","    activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU}[activation_fn]\n","\n","    target_entropy = 'auto'\n","    if ent_coef == 'auto':\n","        target_entropy = trial.suggest_categorical('target_entropy', ['auto', -1, -10, -20, -50, -100])\n","\n","\n","    ## Display true values\n","    # trial.set_user_attr(\"gamma_\", gamma)\n","    # trial.set_user_attr(\"n_steps\", n_steps)\n","\n","\n","    return {\n","        'gamma': gamma,\n","        'learning_rate': learning_rate,\n","        'tau': tau,\n","        #'batch_size': batch_size,\n","        'target_update_interval': target_update_interval,\n","        #'buffer_size': buffer_size,\n","        'learning_starts': learning_starts,\n","        'train_freq': train_freq,\n","        'gradient_steps': gradient_steps,\n","        'ent_coef': ent_coef,\n","        'target_entropy': target_entropy,\n","        'policy_kwargs': {\n","            \"net_arch\": net_arch,\n","            \"activation_fn\": activation_fn\n","        }\n","    }"],"metadata":{"id":"xhBysB7hz_F8","executionInfo":{"status":"aborted","timestamp":1676235823638,"user_tz":360,"elapsed":17,"user":{"displayName":"Cici Du","userId":"17701548280142155870"}}},"id":"xhBysB7hz_F8","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## objective"],"metadata":{"id":"iV2OVSz10Bve"},"id":"iV2OVSz10Bve"},{"cell_type":"code","source":["def objective(trial: optuna.Trial) -> float:\n","    kwargs = DEFAULT_HYPERPARAMS.copy()\n","    # Sample hyperparameters\n","    kwargs.update(sample_sac_params(trial))\n","    # Create the RL model\n","    model = SAC(**kwargs)\n","    # Create env used for evaluation\n","    eval_env = env\n","    # Create the callback that will periodically evaluate\n","    # and report the performance\n","    eval_callback = TrialEvalCallback(\n","      eval_env, trial, n_eval_episodes=N_EVAL_EPISODES, eval_freq=EVAL_FREQ, deterministic=True\n","    )\n","\n","    nan_encountered = False\n","    try:\n","      model.learn(N_TIMESTEPS, callback=eval_callback)\n","    except AssertionError as e:\n","      # Sometimes, random hyperparams can generate NaN\n","      print(e)\n","      nan_encountered = True\n","    finally:\n","      # Free memory\n","      model.env.close()\n","      eval_env.close()\n","\n","    # Tell the optimizer that the trial failed\n","    if nan_encountered:\n","      return float(\"nan\")\n","\n","    if eval_callback.is_pruned:\n","      raise optuna.exceptions.TrialPruned()\n","\n","    return eval_callback.last_mean_reward"],"metadata":{"id":"-3YZk9SVxCWh","executionInfo":{"status":"aborted","timestamp":1676235823638,"user_tz":360,"elapsed":15,"user":{"displayName":"Cici Du","userId":"17701548280142155870"}}},"id":"-3YZk9SVxCWh","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## run"],"metadata":{"id":"UVyTUNFzHtLP"},"id":"UVyTUNFzHtLP"},{"cell_type":"code","source":["env = MultiFF()\n","env.reset()\n","\n","\n","\n","DEFAULT_HYPERPARAMS = {\n","    \"policy\": \"MlpPolicy\",\n","    \"env\": env,\n","}\n","\n","N_TRIALS = 100\n","N_STARTUP_TRIALS = 5\n","N_EVALUATIONS = 2\n","\n","\n","N_TIMESTEPS = 100000\n","EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\n","N_EVAL_EPISODES = 1\n","\n","\n","\n","# Set pytorch num threads to 1 for faster training\n","torch.set_num_threads(1)\n","\n","sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n","# Do not prune before 1/3 of the max budget is used\n","pruner = MedianPruner(n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS//3)\n","\n","study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n","try:\n","    study.optimize(objective, n_trials=N_TRIALS)\n","except KeyboardInterrupt:\n","    pass\n","\n","print(\"Number of finished trials: \", len(study.trials))\n","\n","print(\"Best trial:\")\n","trial = study.best_trial\n","\n","print(\"  Value: \", trial.value)\n","\n","print(\"  Params: \")\n","for key, value in trial.params.items():\n","    print(\"    {}: {}\".format(key, value))\n","\n","print(\"  User attrs:\")\n","for key, value in trial.user_attrs.items():\n","    print(\"    {}: {}\".format(key, value))\n"],"metadata":{"id":"Dhl9rpZPH0w6","executionInfo":{"status":"aborted","timestamp":1676235823639,"user_tz":360,"elapsed":16,"user":{"displayName":"Cici Du","userId":"17701548280142155870"}}},"id":"Dhl9rpZPH0w6","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Animation"],"metadata":{"id":"QvPXVRFfIbVD"},"id":"QvPXVRFfIbVD"},{"cell_type":"markdown","source":["## collect data"],"metadata":{"id":"Z5vjDrdHrSn2"},"id":"Z5vjDrdHrSn2"},{"cell_type":"code","source":["env = CollectInformation()\n","\n","monkey_information, ff_flash_sorted, ff_catched_T_sorted, ff_believed_position_sorted, \\\n","           ff_real_position_sorted, ff_life_sorted, ff_flash_end_sorted, catched_ff_num, total_ff_num, \\\n","           obs_ff_unique_identifiers, sorted_indices_all \\\n","           = collect_agent_data_func(env, sac_model, n_steps = 2000, LSTM = False)\n","\n","\n","ff_dataframe = make_ff_dataframe(monkey_information, ff_catched_T_sorted, ff_flash_sorted,  ff_real_position_sorted, ff_life_sorted, \\\n","                                 player = \"agent\", max_distance = 400, data_folder_name = None, num_missed_index = 0, truncate = False, \\\n","                                 obs_ff_unique_identifiers = obs_ff_unique_identifiers, sorted_indices_all = sorted_indices_all)\n","\n","min_point_index, max_point_index = np.min(np.array(ff_dataframe['point_index'])), np.max(np.array(ff_dataframe['point_index']))\n","\n","n_ff_in_a_row = n_ff_in_a_row_func(ff_believed_position_sorted, distance_between_ff = 50)\n","visible_before_last_one_trials = visible_before_last_one_func(ff_dataframe)\n","disappear_latest_trials = disappear_latest_func(ff_dataframe)\n","ignore_sudden_flash_trials, ignore_sudden_flash_indices, ignore_sudden_flash_indices_for_anim = ignore_sudden_flash_func(ff_dataframe, ff_real_position_sorted, max_point_index)\n","try_a_few_times_trials, try_a_few_times_indices, try_a_few_times_indices_for_anim = try_a_few_times_func(ff_catched_T_sorted, monkey_information, ff_believed_position_sorted, PLAYER, max_point_index)\n","give_up_after_trying_trials, give_up_after_trying_indices, give_up_after_trying_indices_for_anim = give_up_after_trying_func(ff_catched_T_sorted, monkey_information, ff_believed_position_sorted, PLAYER, max_point_index)\n","\n","annotation_info = make_annotation_info(catched_ff_num, max_point_index, n_ff_in_a_row, visible_before_last_one_trials, disappear_latest_trials, \\\n","                                       ignore_sudden_flash_indices, give_up_after_trying_indices, try_a_few_times_indices)"],"metadata":{"id":"yDZgRZMNHnZl","executionInfo":{"status":"aborted","timestamp":1676235823640,"user_tz":360,"elapsed":55842,"user":{"displayName":"Cici Du","userId":"17701548280142155870"}}},"execution_count":null,"outputs":[],"id":"yDZgRZMNHnZl"},{"cell_type":"markdown","source":["## prepare for animation"],"metadata":{"id":"JlxfIO9YFFHe"},"id":"JlxfIO9YFFHe"},{"cell_type":"code","source":["currentTrial = 2\n","num_trials = 7\n","margin = 400\n","k = 5\n","\n","duration = [ff_catched_T_sorted[currentTrial-1], ff_catched_T_sorted[currentTrial+num_trials-1]]\n","cum_index = np.where((monkey_information['monkey_t'] > ff_catched_T_sorted[currentTrial-1]) & \n","                     (monkey_information['monkey_t'] <= ff_catched_T_sorted[currentTrial+num_trials-1]))\n","if len(cum_index) > 0:\n","    anim_monkey_info = make_anim_monkey_info(monkey_information, cum_index, k = k)\n","    flash_on_ff_dict = flash_on_ff_in_trial_by_time(anim_monkey_info['anim_t'], currentTrial, num_trials, ff_flash_sorted, ff_life_sorted, ff_catched_T_sorted)\n","    believed_ff_dict = believed_ff(anim_monkey_info['anim_t'], currentTrial, num_trials, ff_believed_position_sorted, ff_catched_T_sorted)\n","\n","    num_frames = anim_monkey_info['anim_t'].size\n","    ff_position_during_this_trial = np.array([ff_real_position_sorted[ff_index] for ff_index, life in \\\n","                                    enumerate(ff_life_sorted) if (life[-1] >= duration[0]) and (life[0] < duration[1])])\n","else:\n","    print(\"Please try another number for currentTrial, or increase num_trials.\")\n","\n","plt.rcParams['figure.figsize'] = (10, 10)\n","plt.rcParams['font.size'] = 15\n","fig, ax = plt.subplots()"],"metadata":{"id":"Xn7tPJZVHR5B","executionInfo":{"status":"aborted","timestamp":1676235823640,"user_tz":360,"elapsed":55841,"user":{"displayName":"Cici Du","userId":"17701548280142155870"}}},"execution_count":null,"outputs":[],"id":"Xn7tPJZVHR5B"},{"cell_type":"markdown","source":["## make animation"],"metadata":{"id":"AesCxkpsFbVn"},"id":"AesCxkpsFbVn"},{"cell_type":"code","source":["animate_func = partial(animate, ax=ax, anim_monkey_info=anim_monkey_info, ff_dataframe=ff_dataframe, ff_real_position_sorted=ff_real_position_sorted, ff_position_during_this_trial=ff_position_during_this_trial, \\\n","                         flash_on_ff_dict=flash_on_ff_dict, believed_ff_dict=believed_ff_dict, margin = 400)\n","anim = animation.FuncAnimation(fig, animate_func, frames=num_frames, interval=100, repeat=True) \n","HTML(anim.to_html5_video())"],"metadata":{"id":"pUcR1X-0-OQo","executionInfo":{"status":"aborted","timestamp":1676235823640,"user_tz":360,"elapsed":55840,"user":{"displayName":"Cici Du","userId":"17701548280142155870"}}},"execution_count":null,"outputs":[],"id":"pUcR1X-0-OQo"},{"cell_type":"markdown","source":["## make animation with annotation"],"metadata":{"id":"-BX7XwsnFc4k"},"id":"-BX7XwsnFc4k"},{"cell_type":"code","source":["animate_annotated_func = partial(animate_annotated, ax=ax, anim_monkey_info=anim_monkey_info, margin=margin, ff_dataframe=ff_dataframe, ff_real_position_sorted=ff_real_position_sorted, ff_position_during_this_trial=ff_position_during_this_trial, \\\n","                                   flash_on_ff_dict=flash_on_ff_dict, believed_ff_dict=believed_ff_dict, ff_catched_T_sorted=ff_catched_T_sorted, annotation_info=annotation_info)\n","anim_annotated = animation.FuncAnimation(fig, animate_annotated_func, frames=num_frames, interval=100, repeat=True) \n","HTML(anim_annotated.to_html5_video())"],"metadata":{"id":"aE9lp9YZ-H7c","executionInfo":{"status":"aborted","timestamp":1676235823640,"user_tz":360,"elapsed":55839,"user":{"displayName":"Cici Du","userId":"17701548280142155870"}}},"execution_count":null,"outputs":[],"id":"aE9lp9YZ-H7c"},{"cell_type":"markdown","metadata":{"id":"8b62d3c0"},"source":["# Interpret neural network"],"id":"8b62d3c0"},{"cell_type":"markdown","metadata":{"id":"742d2fbf"},"source":["## angle vs. distance, dv"],"id":"742d2fbf"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ecccd41d","executionInfo":{"status":"aborted","timestamp":1676235823640,"user_tz":360,"elapsed":55836,"user":{"displayName":"Cici Du","userId":"17701548280142155870"}}},"outputs":[],"source":["interpret_neural_network_func( sac_model, sample_size = 1000,const_memory = 4, plot_in_xy_coord=True)"],"id":"ecccd41d"},{"cell_type":"markdown","metadata":{"id":"143b64de"},"source":["## angle vs. distance, dw"],"id":"143b64de"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4f41a0d0","executionInfo":{"status":"aborted","timestamp":1676235823640,"user_tz":360,"elapsed":55834,"user":{"displayName":"Cici Du","userId":"17701548280142155870"}}},"outputs":[],"source":["interpret_neural_network_func( sac_model, sample_size = 1000,const_memory = 4, color_variable=\"dw\", plot_in_xy_coord=True)"],"id":"4f41a0d0"},{"cell_type":"markdown","metadata":{"id":"10ec282a"},"source":["## angle vs. memory, dv"],"id":"10ec282a"},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"ab7499b0","executionInfo":{"status":"aborted","timestamp":1676235823641,"user_tz":360,"elapsed":55832,"user":{"displayName":"Cici Du","userId":"17701548280142155870"}}},"outputs":[],"source":["interpret_neural_network_func( sac_model, sample_size = 1000, const_distance = 100, color_variable=\"dv\", plot_in_xy_coord=True)"],"id":"ab7499b0"},{"cell_type":"markdown","metadata":{"id":"a22b3b74"},"source":["## angle vs. memory, dw"],"id":"a22b3b74"},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"850ccdeb","executionInfo":{"status":"aborted","timestamp":1676235823641,"user_tz":360,"elapsed":55830,"user":{"displayName":"Cici Du","userId":"17701548280142155870"}}},"outputs":[],"source":["interpret_neural_network_func( sac_model, sample_size = 1000, const_distance = 100, color_variable=\"dw\", plot_in_xy_coord=True)"],"id":"850ccdeb"},{"cell_type":"markdown","metadata":{"id":"f6c0e036"},"source":["## distance vs. memory, dv"],"id":"f6c0e036"},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"1ecf29d7","executionInfo":{"status":"aborted","timestamp":1676235823641,"user_tz":360,"elapsed":55827,"user":{"displayName":"Cici Du","userId":"17701548280142155870"}}},"outputs":[],"source":["interpret_neural_network_func( sac_model, sample_size = 1000, const_angle = 0.5, color_variable=\"dv\", plot_in_xy_coord=True)"],"id":"1ecf29d7"},{"cell_type":"markdown","metadata":{"id":"8a614a40"},"source":["## distance vs. memory, dw"],"id":"8a614a40"},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"d2713c4f","executionInfo":{"status":"aborted","timestamp":1676235823641,"user_tz":360,"elapsed":55825,"user":{"displayName":"Cici Du","userId":"17701548280142155870"}}},"outputs":[],"source":["interpret_neural_network_func( sac_model, sample_size = 1000, const_angle = 0.5, color_variable=\"dw\", plot_in_xy_coord=True)"],"id":"d2713c4f"}],"metadata":{"kernelspec":{"display_name":"ff_venv","language":"python","name":"ff_venv2"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{},"toc_section_display":true,"toc_window_display":true},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}